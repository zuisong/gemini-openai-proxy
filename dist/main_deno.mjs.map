{
  "version": 3,
  "sources": ["../../../../.cache/deno/npm/registry.npmjs.org/itty-router/src/src/cors.ts", "../../../../.cache/deno/npm/registry.npmjs.org/itty-router/src/src/Router.ts", "../src/gemini-proxy.ts", "../src/utils.ts", "../src/hello.ts", "../src/log.ts", "../../../../.cache/deno/npm/registry.npmjs.org/eventsource-parser/3.0.1/src/errors.ts", "../../../../.cache/deno/npm/registry.npmjs.org/eventsource-parser/3.0.1/src/parse.ts", "../../../../.cache/deno/npm/registry.npmjs.org/eventsource-parser/3.0.1/src/stream.ts", "../src/gemini-api-client/errors.ts", "../src/gemini-api-client/gemini-api-client.ts", "../src/gemini-api-client/response-helper.ts", "../src/openai/chat/completions/NonStreamingChatProxyHandler.ts", "../src/openai/chat/completions/StreamingChatProxyHandler.ts", "../src/openai/chat/completions/ChatProxyHandler.ts", "../src/openai/embeddingProxyHandler.ts", "../src/openai/models.ts", "../src/app.ts", "../main_deno.ts"],
  "sourcesContent": [null, null, "export async function geminiProxy(rawReq: Request) {\n  const url = new URL(rawReq.url)\n  url.host = \"generativelanguage.googleapis.com\"\n  url.port = \"\"\n  url.protocol = \"https:\"\n  const req = new Request(url, rawReq)\n  const resp = await fetch(req)\n  return new Response(resp.body, resp)\n}\n", "import type { Content, GenerateContentRequest, JsonSchema, Part } from \"./gemini-api-client/types.ts\"\nimport type { Any } from \"./log.ts\"\nimport type { OpenAI } from \"./types.ts\"\n\nexport interface ApiParam {\n  apikey: string\n  useBeta: boolean\n}\n\nexport function getToken(headers: Iterable<[string, string]>): ApiParam | null {\n  for (const [k, v] of headers) {\n    if (k.toLowerCase() !== \"authorization\") continue\n\n    const rawApikey = v.substring(v.indexOf(\" \") + 1)\n\n    if (!rawApikey.includes(\"#\")) {\n      return {\n        apikey: rawApikey,\n        useBeta: false,\n      }\n    }\n\n    // todo read config from apikey\n    const apikey = rawApikey.substring(0, rawApikey.indexOf(\"#\"))\n    const params = new URLSearchParams(rawApikey.substring(rawApikey.indexOf(\"#\") + 1))\n    return {\n      apikey,\n      useBeta: params.has(\"useBeta\"),\n    }\n  }\n  return null\n}\n\nfunction parseBase64(base64: string): Part {\n  if (!base64.startsWith(\"data:\")) {\n    return { text: \"\" }\n  }\n\n  const [m, data, ..._arr] = base64.split(\",\")\n  const mimeType = m.match(/:(?<mime>.*?);/)?.groups?.mime ?? \"img/png\"\n  return {\n    inlineData: {\n      mimeType,\n      data,\n    },\n  }\n}\n\nexport function openAiMessageToGeminiMessage(messages: OpenAI.Chat.ChatCompletionMessageParam[]): Content[] {\n  const result: Content[] = messages.flatMap(({ role, content }) => {\n    if (role === \"system\") {\n      return [\n        {\n          role: \"user\",\n          parts: typeof content !== \"string\" ? content : [{ text: content }],\n        },\n      ] satisfies Content[] as Content[]\n    }\n    const parts: Part[] =\n      content == null || typeof content === \"string\"\n        ? [{ text: content?.toString() ?? \"\" }]\n        : content.map((item) => {\n            if (item.type === \"text\") return { text: item.text }\n            if (item.type === \"image_url\") return parseBase64(item.image_url.url)\n            return { text: \"OK\" }\n          })\n    return [{ role: \"user\" === role ? \"user\" : \"model\", parts: parts }]\n  })\n\n  return result\n}\n\nexport function genModel(req: OpenAI.Chat.ChatCompletionCreateParams): [GeminiModel, GenerateContentRequest] {\n  const model: GeminiModel = GeminiModel.modelMapping(req.model)\n\n  let functions: OpenAI.Chat.FunctionObject[] =\n    req.tools?.filter((it) => it.type === \"function\")?.map((it) => it.function) ?? []\n\n  functions = functions.concat((req.functions ?? []).map((it) => ({ strict: null, ...it })))\n\n  const [responseMimeType, responseSchema] = (() => {\n    switch (req.response_format?.type) {\n      case \"json_object\":\n        return [\"application/json\", undefined]\n      case \"json_schema\":\n        return [\"application/json\", req.response_format.json_schema.schema satisfies JsonSchema | undefined]\n      case \"text\":\n        return [\"text/plain\", undefined]\n      default:\n        return [undefined, undefined]\n    }\n  })()\n\n  const generateContentRequest: GenerateContentRequest = {\n    contents: openAiMessageToGeminiMessage(req.messages),\n    generationConfig: {\n      maxOutputTokens: req.max_completion_tokens ?? undefined,\n      temperature: req.temperature ?? undefined,\n      topP: req.top_p ?? undefined,\n      responseMimeType: responseMimeType,\n      responseSchema: responseSchema,\n      thinkingConfig: !model.isThinkingModel()\n        ? undefined\n        : {\n            includeThoughts: true,\n          },\n    },\n    tools:\n      functions.length === 0\n        ? undefined\n        : [\n            {\n              functionDeclarations: functions,\n            },\n          ],\n    safetySettings: (\n      [\n        \"HARM_CATEGORY_HATE_SPEECH\",\n        \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"HARM_CATEGORY_HARASSMENT\",\n      ] as const\n    ).map((category) => ({\n      category,\n      threshold: \"BLOCK_NONE\",\n    })),\n  }\n  return [model, generateContentRequest]\n}\nexport type KnownGeminiModel =\n  | \"gemini-1.5-pro-latest\"\n  | \"gemini-1.5-flash-latest\"\n  | \"gemini-1.5-flash-8b-latest\"\n  | \"gemini-2.0-flash-exp\"\n  | \"text-embedding-004\"\n\nexport type API_VERSION = \"v1beta\" | \"v1\" | \"v1alpha\"\n\nexport class GeminiModel {\n  static modelMapping(model: string | undefined): GeminiModel {\n    const modelName: GeminiModelName | KnownGeminiModel =\n      ModelMapping[model ?? \"\"] ?? GeminiModel.defaultModel(model ?? \"\")\n    return new GeminiModel(modelName)\n  }\n  public readonly model: GeminiModelName\n  constructor(model: GeminiModelName) {\n    this.model = model\n  }\n\n  isThinkingModel(): boolean {\n    return this.model.includes(\"thinking\")\n  }\n\n  apiVersion(): API_VERSION {\n    if (this.isThinkingModel()) {\n      return \"v1alpha\"\n    }\n    return \"v1beta\"\n  }\n\n  toString(): string {\n    return this.model\n  }\n\n  private static defaultModel(m: string): GeminiModelName {\n    if (m.startsWith(\"gemini\")) {\n      return m as GeminiModelName\n    }\n    return \"gemini-1.5-flash-latest\"\n  }\n}\n\nexport type GeminiModelName = `gemini${string}` | \"text-embedding-004\"\n\nexport const ModelMapping: Readonly<Record<string, KnownGeminiModel>> = {\n  \"gpt-3.5-turbo\": \"gemini-1.5-flash-8b-latest\",\n  \"gpt-4\": \"gemini-1.5-pro-latest\",\n  \"gpt-4o\": \"gemini-1.5-flash-latest\",\n  \"gpt-4o-mini\": \"gemini-1.5-flash-8b-latest\",\n  \"gpt-4-vision-preview\": \"gemini-1.5-flash-latest\",\n  \"gpt-4-turbo\": \"gemini-1.5-pro-latest\",\n  \"gpt-4-turbo-preview\": \"gemini-2.0-flash-exp\",\n}\n\nexport function getRuntimeKey() {\n  const global = globalThis as typeof globalThis & Record<string, undefined | Any>\n  if (global?.Deno !== undefined) {\n    return \"deno\"\n  }\n  if (global?.Bun !== undefined) {\n    return \"bun\"\n  }\n  if (typeof global?.WebSocketPair === \"function\") {\n    return \"workerd\"\n  }\n  if (typeof global?.EdgeRuntime === \"string\") {\n    return \"edge-light\"\n  }\n  if (global?.fastly !== undefined) {\n    return \"fastly\"\n  }\n  if (global?.process?.release?.name === \"node\") {\n    return \"node\"\n  }\n  return \"other\"\n}\n", "import { getRuntimeKey } from \"./utils.ts\"\n\nexport function hello(req: Request): Response {\n  const origin = new URL(req.url).origin\n  return new Response(`\n    Hello Gemini-OpenAI-Proxy from ${getRuntimeKey()}!\n\n    You can try it with:\n\n    curl ${origin}/v1/chat/completions \\\\\n    -H \"Authorization: Bearer $YOUR_GEMINI_API_KEY\" \\\\\n    -H \"Content-Type: application/json\" \\\\\n    -d '{\n        \"model\": \"gpt-3.5-turbo\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n        \"temperature\": 0.7\n    }'\n    `)\n}\n", "export type Any = Parameters<typeof console.log>[0]\n\nexport interface ILogger {\n  error: (...data: Any[]) => void\n  warn: (...data: Any[]) => void\n  info: (...data: Any[]) => void\n  debug: (...data: Any[]) => void\n}\n\nconst LEVEL = [\"debug\", \"info\", \"warn\", \"error\"] as const\n\ninterface Config {\n  level: (typeof LEVEL)[number]\n  prefix: string\n}\n\nexport class Logger implements ILogger {\n  private config: Config\n\n  debug!: Log\n  info!: Log\n  warn!: Log\n  error!: Log\n\n  constructor(prefix?: string, logLevel?: string) {\n    const level = LEVEL.find((it) => it === logLevel) ?? \"warn\"\n    this.config = {\n      prefix: prefix ?? \"\",\n      level,\n    }\n\n    for (const m of LEVEL) {\n      this[m] = (...data: Any[]) => this.#write(m, ...data)\n    }\n  }\n\n  #write(level: Config[\"level\"], ...data: Any[]) {\n    const { level: configLevel, prefix } = this.config\n    if (LEVEL.indexOf(level) < LEVEL.indexOf(configLevel)) {\n      return\n    }\n\n    console[level](`${new Date().toISOString()} ${level.toUpperCase()}${prefix ? ` ${prefix}` : \"\"}`, ...data)\n  }\n}\n\ntype Log = typeof console.log\n", "/**\n * The type of error that occurred.\n * @public\n */\nexport type ErrorType = 'invalid-retry' | 'unknown-field'\n\n/**\n * Error thrown when encountering an issue during parsing.\n *\n * @public\n */\nexport class ParseError extends Error {\n  /**\n   * The type of error that occurred.\n   */\n  type: ErrorType\n\n  /**\n   * In the case of an unknown field encountered in the stream, this will be the field name.\n   */\n  field?: string\n\n  /**\n   * In the case of an unknown field encountered in the stream, this will be the value of the field.\n   */\n  value?: string\n\n  /**\n   * The line that caused the error, if available.\n   */\n  line?: string\n\n  constructor(\n    message: string,\n    options: {type: ErrorType; field?: string; value?: string; line?: string},\n  ) {\n    super(message)\n    this.name = 'ParseError'\n    this.type = options.type\n    this.field = options.field\n    this.value = options.value\n    this.line = options.line\n  }\n}\n", "/**\n * EventSource/Server-Sent Events parser\n * @see https://html.spec.whatwg.org/multipage/server-sent-events.html\n */\nimport {ParseError} from './errors.ts'\nimport type {EventSourceParser, ParserCallbacks} from './types.ts'\n\n// eslint-disable-next-line @typescript-eslint/no-unused-vars\nfunction noop(_arg: unknown) {\n  // intentional noop\n}\n\n/**\n * Creates a new EventSource parser.\n *\n * @param callbacks - Callbacks to invoke on different parsing events:\n *   - `onEvent` when a new event is parsed\n *   - `onError` when an error occurs\n *   - `onRetry` when a new reconnection interval has been sent from the server\n *   - `onComment` when a comment is encountered in the stream\n *\n * @returns A new EventSource parser, with `parse` and `reset` methods.\n * @public\n */\nexport function createParser(callbacks: ParserCallbacks): EventSourceParser {\n  if (typeof callbacks === 'function') {\n    throw new TypeError(\n      '`callbacks` must be an object, got a function instead. Did you mean `{onEvent: fn}`?',\n    )\n  }\n\n  const {onEvent = noop, onError = noop, onRetry = noop, onComment} = callbacks\n\n  let incompleteLine = ''\n\n  let isFirstChunk = true\n  let id: string | undefined\n  let data = ''\n  let eventType = ''\n\n  function feed(newChunk: string) {\n    // Strip any UTF8 byte order mark (BOM) at the start of the stream\n    const chunk = isFirstChunk ? newChunk.replace(/^\\xEF\\xBB\\xBF/, '') : newChunk\n\n    // If there was a previous incomplete line, append it to the new chunk,\n    // so we may process it together as a new (hopefully complete) chunk.\n    const [complete, incomplete] = splitLines(`${incompleteLine}${chunk}`)\n\n    for (const line of complete) {\n      parseLine(line)\n    }\n\n    incompleteLine = incomplete\n    isFirstChunk = false\n  }\n\n  function parseLine(line: string) {\n    // If the line is empty (a blank line), dispatch the event\n    if (line === '') {\n      dispatchEvent()\n      return\n    }\n\n    // If the line starts with a U+003A COLON character (:), ignore the line.\n    if (line.startsWith(':')) {\n      if (onComment) {\n        onComment(line.slice(line.startsWith(': ') ? 2 : 1))\n      }\n      return\n    }\n\n    // If the line contains a U+003A COLON character (:)\n    const fieldSeparatorIndex = line.indexOf(':')\n    if (fieldSeparatorIndex !== -1) {\n      // Collect the characters on the line before the first U+003A COLON character (:),\n      // and let `field` be that string.\n      const field = line.slice(0, fieldSeparatorIndex)\n\n      // Collect the characters on the line after the first U+003A COLON character (:),\n      // and let `value` be that string. If value starts with a U+0020 SPACE character,\n      // remove it from value.\n      const offset = line[fieldSeparatorIndex + 1] === ' ' ? 2 : 1\n      const value = line.slice(fieldSeparatorIndex + offset)\n\n      processField(field, value, line)\n      return\n    }\n\n    // Otherwise, the string is not empty but does not contain a U+003A COLON character (:)\n    // Process the field using the whole line as the field name, and an empty string as the field value.\n    // üëÜ This is according to spec. That means that a line that has the value `data` will result in\n    // a newline being added to the current `data` buffer, for instance.\n    processField(line, '', line)\n  }\n\n  function processField(field: string, value: string, line: string) {\n    // Field names must be compared literally, with no case folding performed.\n    switch (field) {\n      case 'event':\n        // Set the `event type` buffer to field value\n        eventType = value\n        break\n      case 'data':\n        // Append the field value to the `data` buffer, then append a single U+000A LINE FEED(LF)\n        // character to the `data` buffer.\n        data = `${data}${value}\\n`\n        break\n      case 'id':\n        // If the field value does not contain U+0000 NULL, then set the `ID` buffer to\n        // the field value. Otherwise, ignore the field.\n        id = value.includes('\\0') ? undefined : value\n        break\n      case 'retry':\n        // If the field value consists of only ASCII digits, then interpret the field value as an\n        // integer in base ten, and set the event stream's reconnection time to that integer.\n        // Otherwise, ignore the field.\n        if (/^\\d+$/.test(value)) {\n          onRetry(parseInt(value, 10))\n        } else {\n          onError(\n            new ParseError(`Invalid \\`retry\\` value: \"${value}\"`, {\n              type: 'invalid-retry',\n              value,\n              line,\n            }),\n          )\n        }\n        break\n      default:\n        // Otherwise, the field is ignored.\n        onError(\n          new ParseError(\n            `Unknown field \"${field.length > 20 ? `${field.slice(0, 20)}‚Ä¶` : field}\"`,\n            {type: 'unknown-field', field, value, line},\n          ),\n        )\n        break\n    }\n  }\n\n  function dispatchEvent() {\n    const shouldDispatch = data.length > 0\n    if (shouldDispatch) {\n      onEvent({\n        id,\n        event: eventType || undefined,\n        // If the data buffer's last character is a U+000A LINE FEED (LF) character,\n        // then remove the last character from the data buffer.\n        data: data.endsWith('\\n') ? data.slice(0, -1) : data,\n      })\n    }\n\n    // Reset for the next event\n    id = undefined\n    data = ''\n    eventType = ''\n  }\n\n  function reset(options: {consume?: boolean} = {}) {\n    if (incompleteLine && options.consume) {\n      parseLine(incompleteLine)\n    }\n\n    isFirstChunk = true\n    id = undefined\n    data = ''\n    eventType = ''\n    incompleteLine = ''\n  }\n\n  return {feed, reset}\n}\n\n/**\n * For the given `chunk`, split it into lines according to spec, and return any remaining incomplete line.\n *\n * @param chunk - The chunk to split into lines\n * @returns A tuple containing an array of complete lines, and any remaining incomplete line\n * @internal\n */\nfunction splitLines(chunk: string): [complete: Array<string>, incomplete: string] {\n  /**\n   * According to the spec, a line is terminated by either:\n   * - U+000D CARRIAGE RETURN U+000A LINE FEED (CRLF) character pair\n   * - a single U+000A LINE FEED(LF) character not preceded by a U+000D CARRIAGE RETURN(CR) character\n   * - a single U+000D CARRIAGE RETURN(CR) character not followed by a U+000A LINE FEED(LF) character\n   */\n  const lines: Array<string> = []\n  let incompleteLine = ''\n  let searchIndex = 0\n\n  while (searchIndex < chunk.length) {\n    // Find next line terminator\n    const crIndex = chunk.indexOf('\\r', searchIndex)\n    const lfIndex = chunk.indexOf('\\n', searchIndex)\n\n    // Determine line end\n    let lineEnd = -1\n    if (crIndex !== -1 && lfIndex !== -1) {\n      // CRLF case\n      lineEnd = Math.min(crIndex, lfIndex)\n    } else if (crIndex !== -1) {\n      lineEnd = crIndex\n    } else if (lfIndex !== -1) {\n      lineEnd = lfIndex\n    }\n\n    // Extract line if terminator found\n    if (lineEnd === -1) {\n      // No terminator found, rest is incomplete\n      incompleteLine = chunk.slice(searchIndex)\n      break\n    } else {\n      const line = chunk.slice(searchIndex, lineEnd)\n      lines.push(line)\n\n      // Move past line terminator\n      searchIndex = lineEnd + 1\n      if (chunk[searchIndex - 1] === '\\r' && chunk[searchIndex] === '\\n') {\n        searchIndex++\n      }\n    }\n  }\n\n  return [lines, incompleteLine]\n}\n", "import {createParser} from './parse.ts'\nimport type {EventSourceMessage, EventSourceParser} from './types.ts'\n\n/**\n * Options for the EventSourceParserStream.\n *\n * @public\n */\nexport interface StreamOptions {\n  /**\n   * Behavior when a parsing error occurs.\n   *\n   * - A custom function can be provided to handle the error.\n   * - `'terminate'` will error the stream and stop parsing.\n   * - Any other value will ignore the error and continue parsing.\n   *\n   * @defaultValue `undefined`\n   */\n  onError?: 'terminate' | ((error: Error) => void)\n\n  /**\n   * Callback for when a reconnection interval is sent from the server.\n   *\n   * @param retry - The number of milliseconds to wait before reconnecting.\n   */\n  onRetry?: (retry: number) => void\n\n  /**\n   * Callback for when a comment is encountered in the stream.\n   *\n   * @param comment - The comment encountered in the stream.\n   */\n  onComment?: (comment: string) => void\n}\n\n/**\n * A TransformStream that ingests a stream of strings and produces a stream of `EventSourceMessage`.\n *\n * @example Basic usage\n * ```\n * const eventStream =\n *   response.body\n *     .pipeThrough(new TextDecoderStream())\n *     .pipeThrough(new EventSourceParserStream())\n * ```\n *\n * @example Terminate stream on parsing errors\n * ```\n * const eventStream =\n *  response.body\n *   .pipeThrough(new TextDecoderStream())\n *   .pipeThrough(new EventSourceParserStream({terminateOnError: true}))\n * ```\n *\n * @public\n */\nexport class EventSourceParserStream extends TransformStream<string, EventSourceMessage> {\n  constructor({onError, onRetry, onComment}: StreamOptions = {}) {\n    let parser!: EventSourceParser\n\n    super({\n      start(controller) {\n        parser = createParser({\n          onEvent: (event) => {\n            controller.enqueue(event)\n          },\n          onError(error) {\n            if (onError === 'terminate') {\n              controller.error(error)\n            } else if (typeof onError === 'function') {\n              onError(error)\n            }\n\n            // Ignore by default\n          },\n          onRetry,\n          onComment,\n        })\n      },\n      transform(chunk) {\n        parser.feed(chunk)\n      },\n    })\n  }\n}\n\nexport {type ErrorType, ParseError} from './errors.ts'\nexport type {EventSourceMessage} from './types.ts'\n", "export class GoogleGenerativeAIError extends Error {\n  constructor(message: string) {\n    super(`[GoogleGenerativeAI Error]: ${message}`)\n  }\n}\n\nexport class GoogleGenerativeAIResponseError<T> extends GoogleGenerativeAIError {\n  public response?: T\n  constructor(message: string, response?: T) {\n    super(message)\n    this.response = response\n  }\n}\n", "import { EventSourceParserStream } from \"eventsource-parser/stream\"\nimport type { components } from \"../generated-types/gemini-types.ts\"\nimport type { ApiParam, GeminiModel } from \"../utils.ts\"\nimport { GoogleGenerativeAIError } from \"./errors.ts\"\nimport type {\n  EmbedContentRequest,\n  EmbedContentResponse,\n  GenerateContentRequest,\n  GenerateContentResponse,\n  RequestOptions,\n} from \"./types.ts\"\n\ninterface Task {\n  streamGenerateContent: {\n    request: GenerateContentRequest\n    response: GenerateContentResponse\n  }\n  embedContent: {\n    request: EmbedContentRequest\n    response: EmbedContentResponse\n  }\n}\n\nexport async function listModels(apiParam: ApiParam | null) {\n  const url = new URL(`${BASE_URL}/v1beta/openai/models`)\n  const resp = await makeRequest(url, undefined, undefined, \"GET\", {\n    Authorization: `Bearer ${apiParam?.apikey ?? \"\"}`,\n  })\n  return (await resp.json()) as components[\"schemas\"][\"ListModelsResponse\"]\n}\nexport async function* streamGenerateContent(\n  apiParam: ApiParam,\n  model: GeminiModel,\n  params: Task[\"streamGenerateContent\"][\"request\"],\n  requestOptions?: RequestOptions,\n) {\n  const response = await makeRequest(\n    toURL({ model, task: \"streamGenerateContent\", stream: true, apiParam }),\n    JSON.stringify(params),\n    requestOptions,\n  )\n  const body = response.body\n  if (body == null) {\n    return\n  }\n\n  for await (const event of body.pipeThrough(new TextDecoderStream()).pipeThrough(new EventSourceParserStream())) {\n    const responseJson = JSON.parse(event.data) as Task[\"streamGenerateContent\"][\"response\"]\n    yield responseJson\n  }\n}\n\nexport async function embedContent(\n  apiParam: ApiParam,\n  model: GeminiModel,\n  params: Task[\"embedContent\"][\"request\"],\n  requestOptions?: RequestOptions,\n) {\n  const response = await makeRequest(\n    toURL({ model, task: \"embedContent\", stream: false, apiParam }),\n    JSON.stringify(params),\n    requestOptions,\n  )\n  const body = response.body\n  if (body == null) {\n    return\n  }\n\n  const responseJson = (await response.json()) as Task[\"embedContent\"][\"response\"]\n  return responseJson\n}\n\nasync function makeRequest(\n  url: URL,\n  body: string | undefined,\n  requestOptions?: RequestOptions,\n  requestMethod = \"POST\",\n  headers: Record<string, string> = {},\n): Promise<Response> {\n  let response: Response\n  try {\n    response = await fetch(url, {\n      ...buildFetchOptions(requestOptions),\n      method: requestMethod,\n      headers: {\n        \"Content-Type\": \"application/json\",\n        ...headers,\n      },\n      body,\n    })\n    if (!response.ok) {\n      let message: string | undefined = \"\"\n      try {\n        const errResp = (await response.json()) as components[\"schemas\"][\"Operation\"]\n        message = errResp.error?.message\n        if (errResp?.error?.details) {\n          message += ` ${JSON.stringify(errResp.error.details)}`\n        }\n      } catch (_e) {\n        // ignored\n      }\n      throw new Error(`[${response.status} ${response.statusText}] ${message}`)\n    }\n  } catch (e) {\n    console.log(e)\n    const err = new GoogleGenerativeAIError(`Error fetching from google -> ${e.message}`)\n    err.stack = e.stack\n    throw err\n  }\n  return response\n}\n\nconst BASE_URL = \"https://generativelanguage.googleapis.com\"\n\nfunction toURL({\n  model,\n  task,\n  stream,\n  apiParam,\n}: {\n  model: GeminiModel\n  task: keyof Task\n  stream: boolean\n  apiParam: ApiParam\n}) {\n  const api_version = model.apiVersion()\n  const url = new URL(`${BASE_URL}/${api_version}/models/${model}:${task}`)\n  url.searchParams.append(\"key\", apiParam.apikey)\n  if (stream) {\n    url.searchParams.append(\"alt\", \"sse\")\n  }\n  return url\n}\n\n/**\n * Generates the request options to be passed to the fetch API.\n * @param requestOptions - The user-defined request options.\n * @returns The generated request options.\n */\nfunction buildFetchOptions(requestOptions?: RequestOptions): RequestInit {\n  const fetchOptions = {} as RequestInit\n  if (requestOptions?.timeout) {\n    const abortController = new AbortController()\n    const signal = abortController.signal\n    setTimeout(() => abortController.abort(), requestOptions.timeout)\n    fetchOptions.signal = signal\n  }\n  return fetchOptions\n}\n", "import { GoogleGenerativeAIResponseError } from \"./errors.ts\"\nimport type { Candidate, FinishReason, FunctionCall, GenerateContentResponse } from \"./types.ts\"\n\n/**\n * Adds convenience helper methods to a response object, including stream\n * chunks (as long as each chunk is a complete GenerateContentResponse JSON).\n */\nexport function resultHelper(response: GenerateContentResponse): string | FunctionCall {\n  if (response.candidates && response.candidates.length > 0) {\n    if (response.candidates.length > 1) {\n      console.warn(\n        `This response had ${response.candidates.length} candidates. Returning text from the first candidate only. Access response.candidates directly to use the other candidates.`,\n      )\n    }\n    if (hadBadFinishReason(response.candidates[0])) {\n      throw new GoogleGenerativeAIResponseError<GenerateContentResponse>(\n        `${formatBlockErrorMessage(response)}`,\n        response,\n      )\n    }\n    return getText(response)\n  }\n  if (response.promptFeedback) {\n    throw new GoogleGenerativeAIResponseError<GenerateContentResponse>(\n      `Text not available. ${formatBlockErrorMessage(response)}`,\n      response,\n    )\n  }\n  return \"\"\n}\n\n/**\n * Returns text of first candidate.\n */\nexport function getText(response: GenerateContentResponse): string | FunctionCall {\n  if (response.candidates?.[0].content?.parts?.[0]?.text) {\n    return response.candidates[0].content.parts[0].text\n  }\n  if (response.candidates?.[0].content?.parts?.[0]?.functionCall) {\n    return response.candidates[0].content.parts[0].functionCall\n  }\n  return \"\"\n}\n\nconst badFinishReasons: FinishReason[] = [\"RECITATION\", \"SAFETY\"]\n\nfunction hadBadFinishReason(candidate: Candidate): boolean {\n  return !!candidate.finishReason && badFinishReasons.includes(candidate.finishReason)\n}\n\nfunction formatBlockErrorMessage(response: GenerateContentResponse): string {\n  let message = \"\"\n  if ((!response.candidates || response.candidates.length === 0) && response.promptFeedback) {\n    message += \"Response was blocked\"\n    if (response.promptFeedback?.blockReason) {\n      message += ` due to ${response.promptFeedback.blockReason}`\n    }\n    if (response.promptFeedback?.blockReasonMessage) {\n      message += `: ${response.promptFeedback.blockReasonMessage}`\n    }\n  } else if (response.candidates?.[0]) {\n    const firstCandidate = response.candidates[0]\n    if (hadBadFinishReason(firstCandidate)) {\n      message += `Candidate was blocked due to ${firstCandidate.finishReason}`\n      if (firstCandidate.finishMessage) {\n        message += `: ${firstCandidate.finishMessage}`\n      }\n    }\n  }\n  return message\n}\n", "import { streamGenerateContent } from \"../../../gemini-api-client/gemini-api-client.ts\"\nimport { resultHelper } from \"../../../gemini-api-client/response-helper.ts\"\nimport type { FunctionCall } from \"../../../gemini-api-client/types.ts\"\nimport type { Logger } from \"../../../log.ts\"\nimport type { OpenAI } from \"../../../types.ts\"\nimport { type ApiParam, genModel } from \"../../../utils.ts\"\n\nexport async function nonStreamingChatProxyHandler(\n  req: OpenAI.Chat.ChatCompletionCreateParams,\n  apiParam: ApiParam,\n  log?: Logger,\n): Promise<Response> {\n  const [model, geminiReq] = genModel(req)\n  let geminiResp: string | FunctionCall = \"\"\n\n  try {\n    for await (const it of streamGenerateContent(apiParam, model, geminiReq)) {\n      const data = resultHelper(it)\n      if (typeof data === \"string\") {\n        geminiResp += data\n      } else {\n        geminiResp = data\n        break\n      }\n    }\n  } catch (err) {\n    // Âá∫Áé∞ÂºÇÂ∏∏Êó∂ÊâìÂç∞ËØ∑Ê±ÇÂèÇÊï∞ÂíåÂìçÂ∫îÔºå‰ª•‰æøË∞ÉËØï\n    log?.error(req)\n    log?.error(err?.message ?? err.toString())\n    geminiResp = err?.message ?? err.toString()\n  }\n\n  log?.debug(req)\n  log?.debug(geminiResp)\n\n  function genOpenAiResp(content: string | FunctionCall): OpenAI.Chat.ChatCompletion {\n    if (typeof content === \"string\") {\n      return {\n        id: \"chatcmpl-abc123\",\n        object: \"chat.completion\",\n        created: Math.floor(Date.now() / 1000),\n        model: model.model,\n        choices: [\n          {\n            message: { role: \"assistant\", content: content, refusal: null },\n            finish_reason: \"stop\",\n            index: 0,\n            logprobs: null,\n          },\n        ],\n      }\n    }\n\n    return {\n      id: \"chatcmpl-abc123\",\n      object: \"chat.completion\",\n      created: Math.floor(Date.now() / 1000),\n      model: model.model,\n      choices: [\n        {\n          message: {\n            role: \"assistant\",\n            refusal: null,\n            content: null,\n            function_call: {\n              name: content.name ?? \"\",\n              arguments: JSON.stringify(content.args),\n            },\n          },\n          finish_reason: \"function_call\",\n          index: 0,\n          logprobs: null,\n        },\n      ],\n    }\n  }\n\n  return Response.json(genOpenAiResp(geminiResp))\n}\n", "import { streamGenerateContent } from \"../../../gemini-api-client/gemini-api-client.ts\"\nimport { resultHelper } from \"../../../gemini-api-client/response-helper.ts\"\nimport type { FunctionCall } from \"../../../gemini-api-client/types.ts\"\nimport type { Logger } from \"../../../log.ts\"\nimport type { OpenAI } from \"../../../types.ts\"\nimport { type ApiParam, genModel } from \"../../../utils.ts\"\n\nexport function streamingChatProxyHandler(\n  req: OpenAI.Chat.ChatCompletionCreateParams,\n  apiParam: ApiParam,\n  log?: Logger,\n): Response {\n  const [model, geminiReq] = genModel(req)\n  log?.debug(\"streamGenerateContent request\", req)\n  return sseResponse(\n    (async function* () {\n      try {\n        for await (const it of streamGenerateContent(apiParam, model, geminiReq)) {\n          log?.debug(\"streamGenerateContent resp\", it)\n          const data = resultHelper(it)\n          yield genStreamResp({\n            model: model.model,\n            content: data,\n            stop: false,\n          })\n        }\n      } catch (error) {\n        yield genStreamResp({\n          model: model.model,\n          content: error?.message ?? error.toString(),\n          stop: true,\n        })\n      }\n      yield genStreamResp({ model: model.model, content: \"\", stop: true })\n      yield \"[DONE]\"\n      return undefined\n    })(),\n  )\n}\n\nfunction genStreamResp({\n  model,\n  content,\n  stop,\n}: {\n  model: string\n  content: string | FunctionCall\n  stop: boolean\n}): OpenAI.Chat.ChatCompletionChunk {\n  if (typeof content === \"string\") {\n    return {\n      id: \"chatcmpl-abc123\",\n      object: \"chat.completion.chunk\",\n      created: Math.floor(Date.now() / 1000),\n      model: model,\n      choices: [\n        {\n          delta: { role: \"assistant\", content },\n          finish_reason: stop ? \"stop\" : null,\n          index: 0,\n        },\n      ],\n    } satisfies OpenAI.Chat.ChatCompletionChunk\n  }\n\n  return {\n    id: \"chatcmpl-abc123\",\n    object: \"chat.completion.chunk\",\n    created: Math.floor(Date.now() / 1000),\n    model: model,\n    choices: [\n      {\n        delta: { role: \"assistant\", function_call: content },\n        finish_reason: stop ? \"function_call\" : null,\n        index: 0,\n      },\n    ],\n  } satisfies OpenAI.Chat.ChatCompletionChunk\n}\n\nconst encoder = new TextEncoder()\n\nfunction sseResponse(dataStream: AsyncGenerator<string | OpenAI.Chat.ChatCompletionChunk, undefined>): Response {\n  const s = new ReadableStream<Uint8Array>({\n    async pull(controller) {\n      const { value, done } = await dataStream.next()\n      if (done) {\n        controller.close()\n      } else {\n        const data = typeof value === \"string\" ? value : JSON.stringify(value)\n        controller.enqueue(encoder.encode(toSseMsg({ data })))\n      }\n    },\n  })\n\n  const response = new Response(s, {\n    status: 200,\n    headers: new Headers({\n      \"Content-Type\": \"text/event-stream\",\n    }),\n  })\n\n  return response\n}\n\nexport function toSseMsg({ event, data, id }: SseEvent) {\n  let result = `data: ${data}\\n`\n  if (event) {\n    result += `event: ${event ?? \"\"}\\n`\n  }\n  if (id) {\n    result += `id: ${id ?? \"\"}\\n`\n  }\n  return `${result}\\n`\n}\n\nexport interface SseEvent {\n  event?: string\n  id?: string\n  data: string\n}\n", "import type { OpenAI } from \"../../../types.ts\"\nimport { getToken } from \"../../../utils.ts\"\nimport { nonStreamingChatProxyHandler } from \"./NonStreamingChatProxyHandler.ts\"\nimport { streamingChatProxyHandler } from \"./StreamingChatProxyHandler.ts\"\n\nexport async function chatProxyHandler(rawReq: Request): Promise<Response> {\n  const req = (await rawReq.json()) as OpenAI.Chat.ChatCompletionCreateParams\n  const headers = rawReq.headers\n  const apiParam = getToken(headers)\n  if (apiParam == null) {\n    return new Response(\"Unauthorized\", { status: 401 })\n  }\n\n  if (req.stream !== true) {\n    return await nonStreamingChatProxyHandler(req, apiParam, rawReq.logger)\n  }\n  return streamingChatProxyHandler(req, apiParam, rawReq.logger)\n}\n", "import { embedContent } from \"../gemini-api-client/gemini-api-client.ts\"\nimport type { EmbedContentRequest } from \"../gemini-api-client/types.ts\"\nimport type { OpenAI } from \"../types.ts\"\nimport { GeminiModel, getToken } from \"../utils.ts\"\n\nexport async function embeddingProxyHandler(rawReq: Request): Promise<Response> {\n  const req = (await rawReq.json()) as OpenAI.Embeddings.EmbeddingCreateParams\n  const log = rawReq.logger\n  const headers = rawReq.headers\n  const apiParam = getToken(headers)\n  if (apiParam == null) {\n    return new Response(\"Unauthorized\", { status: 401 })\n  }\n\n  const embedContentRequest: EmbedContentRequest = {\n    model: \"models/text-embedding-004\",\n    content: {\n      parts: [req.input].flat().map((it) => ({ text: it.toString() })),\n    },\n  }\n\n  log?.warn(\"request\", embedContentRequest)\n\n  let geminiResp: number[] | undefined = []\n\n  try {\n    const it = await embedContent(apiParam, new GeminiModel(\"text-embedding-004\"), embedContentRequest)\n    const data = it?.embedding?.values\n    geminiResp = data\n  } catch (err) {\n    // Âá∫Áé∞ÂºÇÂ∏∏Êó∂ÊâìÂç∞ËØ∑Ê±ÇÂèÇÊï∞ÂíåÂìçÂ∫îÔºå‰ª•‰æøË∞ÉËØï\n    log?.error(req)\n    log?.error(err?.message ?? err.toString())\n    geminiResp = err?.message ?? err.toString()\n  }\n\n  log?.debug(req)\n  log?.debug(geminiResp)\n\n  const resp: OpenAI.Embeddings.CreateEmbeddingResponse = {\n    object: \"list\",\n    data: [\n      {\n        object: \"embedding\",\n        index: 0,\n        embedding: geminiResp ?? [],\n      },\n    ],\n    model: req.model,\n    usage: {\n      prompt_tokens: 5,\n      total_tokens: 5,\n    },\n  }\n\n  return Response.json(resp)\n}\n", "import { listModels } from \"../gemini-api-client/gemini-api-client.ts\"\nimport type { OpenAI } from \"../types.ts\"\nimport { getToken, ModelMapping } from \"../utils.ts\"\nexport const modelData: OpenAI.Models.Model[] = Object.keys(ModelMapping).map((model) => ({\n  created: 1677610602,\n  object: \"model\",\n  owned_by: \"openai\",\n  id: model,\n}))\n\nexport const models = async (req: Request) => {\n  const apiParam = getToken(req.headers)\n  return await listModels(apiParam)\n\n  // return {\n  //   object: \"list\",\n  //   data: modelData,\n  // }\n}\n\nexport const modelDetail = (model: string) => {\n  return modelData.find((it) => it.id === model)\n}\n", "import type { IRequest } from \"itty-router/\"\nimport { cors } from \"itty-router/cors\"\nimport { Router } from \"itty-router/Router\"\nimport { geminiProxy } from \"./gemini-proxy.ts\"\nimport { hello } from \"./hello.ts\"\nimport { type Any, Logger } from \"./log.ts\"\nimport { chatProxyHandler } from \"./openai/chat/completions/ChatProxyHandler.ts\"\nimport { embeddingProxyHandler } from \"./openai/embeddingProxyHandler.ts\"\nimport { modelDetail, models } from \"./openai/models.ts\"\n\nconst { preflight, corsify } = cors({ allowHeaders: \"*\" })\n\nconst app = Router<IRequest, Any[], Response>({\n  before: [\n    preflight,\n    (req) => {\n      req.logger = new Logger(crypto.randomUUID().toString())\n      req.logger.warn(`--> ${req.method} ${req.url}`)\n    },\n  ],\n  finally: [\n    corsify,\n    (_, req) => {\n      req.logger?.warn(`<-- ${req.method} ${req.url}`)\n      // return resp\n    },\n  ],\n})\n\napp.get(\"/\", hello)\napp.post(\"/v1/chat/completions\", chatProxyHandler)\napp.post(\"/v1/embeddings\", embeddingProxyHandler)\napp.get(\"/v1/models\", async (req) => Response.json(await models(req)))\napp.get(\"/v1/models/:model\", (c) => Response.json(modelDetail(c.params.model)))\napp.post(\"/:model_version/models/:model_and_action\", geminiProxy)\napp.all(\"*\", () => new Response(\"Page Not Found\", { status: 404 }))\n\nexport { app }\n", "import { app } from \"./src/app.ts\"\n\nDeno.serve({ port: 8000 }, app.fetch)\n"],
  "mappings": ";IAoBaA,IAAQC,CAAuBC,KAAA,CAAA,MAE1C;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ICdWC,IAITC,CAAAA,EAAAA,MAAOC,KAAIC,IAAAA,QAAgBC,KAA4C,CAAA,GAAA,GAEvEC,EAAAA,IAAAA,CAAAA,OAAW;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ACdf,eAAsB,YAAY,QAAe;AAC/C,QAAM,MAAM,IAAI,IAAI,OAAO,GAAG;AAC9B,MAAI,OAAO;AACX,MAAI,OAAO;AACX,MAAI,WAAW;AACf,QAAM,MAAM,IAAI,QAAQ,KAAK,MAAA;AAC7B,QAAM,OAAO,MAAM,MAAM,GAAA;AACzB,SAAO,IAAI,SAAS,KAAK,MAAM,IAAA;AACjC;;;ACCO,SAAS,SAAS,SAAmC;AAC1D,aAAW,CAAC,GAAG,CAAA,KAAM,SAAS;AAC5B,QAAI,EAAE,YAAW,MAAO,gBAAiB;AAEzC,UAAM,YAAY,EAAE,UAAU,EAAE,QAAQ,GAAA,IAAO,CAAA;AAE/C,QAAI,CAAC,UAAU,SAAS,GAAA,GAAM;AAC5B,aAAO;QACL,QAAQ;QACR,SAAS;MACX;IACF;AAGA,UAAM,SAAS,UAAU,UAAU,GAAG,UAAU,QAAQ,GAAA,CAAA;AACxD,UAAM,SAAS,IAAI,gBAAgB,UAAU,UAAU,UAAU,QAAQ,GAAA,IAAO,CAAA,CAAA;AAChF,WAAO;MACL;MACA,SAAS,OAAO,IAAI,SAAA;IACtB;EACF;AACA,SAAO;AACT;AAEA,SAAS,YAAY,QAAc;AACjC,MAAI,CAAC,OAAO,WAAW,OAAA,GAAU;AAC/B,WAAO;MAAE,MAAM;IAAG;EACpB;AAEA,QAAM,CAAC,GAAG,MAAM,GAAG,IAAA,IAAQ,OAAO,MAAM,GAAA;AACxC,QAAM,WAAW,EAAE,MAAM,gBAAA,GAAmB,QAAQ,QAAQ;AAC5D,SAAO;IACL,YAAY;MACV;MACA;IACF;EACF;AACF;AAEO,SAAS,6BAA6B,UAAkD;AAC7F,QAAM,SAAoB,SAAS,QAAQ,CAAC,EAAE,MAAM,QAAO,MAAE;AAC3D,QAAI,SAAS,UAAU;AACrB,aAAO;QACL;UACE,MAAM;UACN,OAAO,OAAO,YAAY,WAAW,UAAU;YAAC;cAAE,MAAM;YAAQ;;QAClE;;IAEJ;AACA,UAAM,QACJ,WAAW,QAAQ,OAAO,YAAY,WAClC;MAAC;QAAE,MAAM,SAAS,SAAA,KAAc;MAAG;QACnC,QAAQ,IAAI,CAAC,SAAA;AACX,UAAI,KAAK,SAAS,OAAQ,QAAO;QAAE,MAAM,KAAK;MAAK;AACnD,UAAI,KAAK,SAAS,YAAa,QAAO,YAAY,KAAK,UAAU,GAAG;AACpE,aAAO;QAAE,MAAM;MAAK;IACtB,CAAA;AACN,WAAO;MAAC;QAAE,MAAM,WAAW,OAAO,SAAS;QAAS;MAAa;;EACnE,CAAA;AAEA,SAAO;AACT;AAEO,SAAS,SAAS,KAA2C;AAClE,QAAM,QAAqB,YAAY,aAAa,IAAI,KAAK;AAE7D,MAAI,YACF,IAAI,OAAO,OAAO,CAAC,OAAO,GAAG,SAAS,UAAA,GAAa,IAAI,CAAC,OAAO,GAAG,QAAQ,KAAK,CAAA;AAEjF,cAAY,UAAU,QAAQ,IAAI,aAAa,CAAA,GAAI,IAAI,CAAC,QAAQ;IAAE,QAAQ;IAAM,GAAG;EAAG,EAAC,CAAA;AAEvF,QAAM,CAAC,kBAAkB,cAAA,KAAmB,MAAA;AAC1C,YAAQ,IAAI,iBAAiB,MAAA;MAC3B,KAAK;AACH,eAAO;UAAC;UAAoB;;MAC9B,KAAK;AACH,eAAO;UAAC;UAAoB,IAAI,gBAAgB,YAAY;;MAC9D,KAAK;AACH,eAAO;UAAC;UAAc;;MACxB;AACE,eAAO;UAAC;UAAW;;IACvB;EACF,GAAC;AAED,QAAM,yBAAiD;IACrD,UAAU,6BAA6B,IAAI,QAAQ;IACnD,kBAAkB;MAChB,iBAAiB,IAAI,yBAAyB;MAC9C,aAAa,IAAI,eAAe;MAChC,MAAM,IAAI,SAAS;MACnB;MACA;MACA,gBAAgB,CAAC,MAAM,gBAAe,IAClC,SACA;QACE,iBAAiB;MACnB;IACN;IACA,OACE,UAAU,WAAW,IACjB,SACA;MACE;QACE,sBAAsB;MACxB;;IAER,gBACE;MACE;MACA;MACA;MACA;MAEF,IAAI,CAAC,cAAc;MACnB;MACA,WAAW;IACb,EAAC;EACH;AACA,SAAO;IAAC;IAAO;;AACjB;AAUO,IAAM,cAAN,MAAM,aAAA;EACX,OAAO,aAAa,OAAwC;AAC1D,UAAM,YACJ,aAAa,SAAS,EAAA,KAAO,aAAY,aAAa,SAAS,EAAA;AACjE,WAAO,IAAI,aAAY,SAAA;EACzB;EACgB;EAChB,YAAY,OAAwB;AAClC,SAAK,QAAQ;EACf;EAEA,kBAA2B;AACzB,WAAO,KAAK,MAAM,SAAS,UAAA;EAC7B;EAEA,aAA0B;AACxB,QAAI,KAAK,gBAAe,GAAI;AAC1B,aAAO;IACT;AACA,WAAO;EACT;EAEA,WAAmB;AACjB,WAAO,KAAK;EACd;EAEA,OAAe,aAAa,GAA4B;AACtD,QAAI,EAAE,WAAW,QAAA,GAAW;AAC1B,aAAO;IACT;AACA,WAAO;EACT;AACF;AAIO,IAAM,eAA2D;EACtE,iBAAiB;EACjB,SAAS;EACT,UAAU;EACV,eAAe;EACf,wBAAwB;EACxB,eAAe;EACf,uBAAuB;AACzB;AAEO,SAAS,gBAAA;AACd,QAAM,SAAS;AACf,MAAI,QAAQ,SAAS,QAAW;AAC9B,WAAO;EACT;AACA,MAAI,QAAQ,QAAQ,QAAW;AAC7B,WAAO;EACT;AACA,MAAI,OAAO,QAAQ,kBAAkB,YAAY;AAC/C,WAAO;EACT;AACA,MAAI,OAAO,QAAQ,gBAAgB,UAAU;AAC3C,WAAO;EACT;AACA,MAAI,QAAQ,WAAW,QAAW;AAChC,WAAO;EACT;AACA,MAAI,QAAQ,SAAS,SAAS,SAAS,QAAQ;AAC7C,WAAO;EACT;AACA,SAAO;AACT;;;AC3MO,SAAS,MAAM,KAAY;AAChC,QAAM,SAAS,IAAI,IAAI,IAAI,GAAG,EAAE;AAChC,SAAO,IAAI,SAAS;qCACe,cAAA,CAAA;;;;WAI1B,MAAA;;;;;;;;KAQN;AACL;;;ACTA,IAAM,QAAQ;EAAC;EAAS;EAAQ;EAAQ;;AAOjC,IAAM,SAAN,MAAM;EACH;EAER;EACA;EACA;EACA;EAEA,YAAY,QAAiB,UAAmB;AAC9C,UAAM,QAAQ,MAAM,KAAK,CAAC,OAAO,OAAO,QAAA,KAAa;AACrD,SAAK,SAAS;MACZ,QAAQ,UAAU;MAClB;IACF;AAEA,eAAW,KAAK,OAAO;AACrB,WAAK,CAAA,IAAK,IAAI,SAAgB,KAAK,OAAO,GAAA,GAAM,IAAA;IAClD;EACF;EAEA,OAAO,UAA2B,MAAW;AAC3C,UAAM,EAAE,OAAO,aAAa,OAAM,IAAK,KAAK;AAC5C,QAAI,MAAM,QAAQ,KAAA,IAAS,MAAM,QAAQ,WAAA,GAAc;AACrD;IACF;AAEA,YAAQ,KAAA,EAAO,IAAG,oBAAI,KAAA,GAAO,YAAW,CAAA,IAAM,MAAM,YAAW,CAAA,GAAK,SAAS,IAAI,MAAA,KAAW,EAAA,IAAI,GAAK,IAAA;EACvG;AACF;;;ACjCO,IAAM,aAAN,cAAyB,MAAM;EAqBpC,YACE,SACA,SAAA;AAEA,UAAM,OAAO,GACb,KAAK,OAAO,cACZ,KAAK,OAAO,QAAQ,MACpB,KAAK,QAAQ,QAAQ,OACrB,KAAK,QAAQ,QAAQ,OACrB,KAAK,OAAO,QAAQ;EACtB;AACF;ACnCA,SAAS,KAAK,MAAe;AAAA;AAE7B,SAAA,aAAA,WAAA;AAcO,MAAA,OAAS,aAAa,WAA+C,OAAA,IAAA,UAAA,sFAAA;AAC1E,QAAI,EAAA,UAAO,MAAc,UAAA,MAAA,UAAA,MAAA,UAAA,IAAA;AACvB,MAAA,iBAAU,IAAA,eAAA,MAAA,IAAA,OAAA,IAAA,YAAA;AAAA,WACR,KAAA,UAAA;AAAA,UAAA,QAAA,eAAA,SAAA,QAAA,iBAAA,EAAA,IAAA,UAAA,CAAA,UAAA,UAAA,IAAA,WAAA,GAAA,cAAA,GAAA,KAAA,EAAA;AAIE,eAAC,QAAU,SAAM,WAAgB,IAAA;AAEvC,qBAAI,YAEA,eACA;EAIJ;AAEE,WAAM,UAAQ,MAAA;AAMd,QAAA,SAAW,IAAA;AACT,oBAAc;AAGhB;IAEF;AAEA,QAAA,KAAS,WAAU,GAAc,GAAA;AAE/B,mBAAa,UAAI,KAAA,MAAA,KAAA,WAAA,IAAA,IAAA,IAAA,CAAA,CAAA;AACD;IACd;AACF,UAAA,sBAAA,KAAA,QAAA,GAAA;AAGI,QAAA,wBAAsB,IAAA;AACpB,YAAA,QACF,KAAA,MAAU,GAAK,mBAAW,GAAW,SAAQ,KAAM,sBAAA,CAAA,MAAA,MAAA,IAAA,GAAA,QAAA,KAAA,MAAA,sBAAA,MAAA;AAErD,mBAAA,OAAA,OAAA,IAAA;AACF;IAGM;AACN,iBAAI,MAAA,IAAA,IAAA;EAGF;AAQa,WAAA,aAAA,OAAO,OAAW,MAAA;AAC/B,YAAA,OAAA;MACF,KAAA;AAMa,oBAAA;AACf;MAES,KAAA;AAEP,eAAQ,GAAA,IAAO,GAAA,KAAA;;AAGC;MACZ,KAAA;AACF,aAAK,MAAA,SAAA,IAAA,IAAA,SAAA;AAGI;MAAe,KAAA;AACtB,gBAAA,KAAA,KAAA,IAAA,QAAA,SAAA,OAAA,EAAA,CAAA,IAAA,QAAA,IAAA,WAAA,6BAAA,KAAA,KAAA;UACF,MAAK;UAGH;UACA;QACF,CAAA,CAAA;AAIM;MAGF;AACwD,gBACpD,IAAM,WAAA,kBAAA,MAAA,SAAA,KAAA,GAAA,MAAA,MAAA,GAAA,EAAA,CAAA,WAAA,KAAA,KAAA;UAAA,MACN;UAAA;UACA;UACD;QAGL,CAAA,CAAA;AACF;IAEE;EAAA;AACM,WAAA,gBACF;AAAsE,SAAA,SACrE,KAAM,QAAA;MAAmC;MAC5C,OAAA,aAAA;;;MAIR,MAAA,KAAA,SAAA;CAEA,IAAA,KAAA,MAAS,GAAA,EAAA,IAAA;IACgB,CAAA,GAAA,KAAK,QAAS,OAEnC,IAAQ,YAAA;EAAA;AACN,WACA,MAAO,UAAa,CAAA,GAAA;AAAA,sBAAA,QAAA,WAAA,UAAA,cAAA,GAAA,eAAA,MAAA,KAAA,QAAA,OAAA,IAAA,YAAA,IAAA,iBAAA;EAAA;AAAA,SAGpB;IAAwB;IAAwB;EAQtD;AAES;AACH,SAAA,WAAA,OAAkB;AASxB,QAAA,QAAA,CAAA;AAEO,MAAA,iBAAO,IAAA,cAAA;AAChB,SAAA,cAAA,MAAA,UAAA;AASA,UAAS,UAAW,MAAA,QAA8D,MAAA,WAAA,GAAA,UAAA,MAAA,QAAA;GAOhF,WAAM;AACF,QAAA,UAAA;AAGG,QAAA,YAAA,MAAc,YAAM,KAAQ,UAAA,KAAA,IAAA,SAAA,OAAA,IAAA,YAAA,KAAA,UAAA,UAAA,YAAA,OAAA,UAAA,UAAA,YAAA,IAAA;AAE3B,uBAAU,MAAM,MAAQ,WAAM;AACA;IAGpC,OAAI;AAWJ,YAVI,OAAA,MAAY,MAAM,aAEpB,OAAA;AAUiB,YAAA,KAAA,IAAA,GAAA,cAAY,UAAW,GAAA,MAAA,cAAA,CAAA,MAAA,QAAA,MAAA,WAAA,MAAA;KACxC;IAAA;EAEA;AACA,SAAA;IAI8D;IAGhE;EACF;AAEO;;;ACxKF,IAAM,0BAAN,cAAsC,gBAA4C;EACvF,YAAY,EAAC,SAAS,SAAS,UAAS,IAAmB,CAAA,GAAA;AACrD,QAAA;AAEE,UAAA;MACJ,MAAM,YAAY;AAChB,iBAAS,aAAa;UACpB,SAAS,CAAC,UAAA;AACR,uBAAW,QAAQ,KAAK;UAC1B;UACA,QAAQ,OAAO;AACT,wBAAY,cACd,WAAW,MAAM,KAAK,IACb,OAAO,WAAY,cAC5B,QAAQ,KAAK;UAIjB;UACA;UACA;QAAA,CACD;MACH;MACA,UAAU,OAAO;AACf,eAAO,KAAK,KAAK;MACnB;IAAA,CACD;EACH;AACF;;;ACpFO,IAAM,0BAAN,cAAsC,MAAA;EAC3C,YAAY,SAAiB;AAC3B,UAAM,+BAA+B,OAAA,EAAS;EAChD;AACF;AAEO,IAAM,kCAAN,cAAiD,wBAAA;EAC/C;EACP,YAAY,SAAiB,UAAc;AACzC,UAAM,OAAA;AACN,SAAK,WAAW;EAClB;AACF;;;ACWA,eAAsB,WAAW,UAAyB;AACxD,QAAM,MAAM,IAAI,IAAI,GAAG,QAAA,uBAA+B;AACtD,QAAM,OAAO,MAAM,YAAY,KAAK,QAAW,QAAW,OAAO;IAC/D,eAAe,UAAU,UAAU,UAAU,EAAA;EAC/C,CAAA;AACA,SAAQ,MAAM,KAAK,KAAI;AACzB;AACA,gBAAuB,sBACrB,UACA,OACA,QACA,gBAA+B;AAE/B,QAAM,WAAW,MAAM,YACrB,MAAM;IAAE;IAAO,MAAM;IAAyB,QAAQ;IAAM;EAAS,CAAA,GACrE,KAAK,UAAU,MAAA,GACf,cAAA;AAEF,QAAM,OAAO,SAAS;AACtB,MAAI,QAAQ,MAAM;AAChB;EACF;AAEA,mBAAiB,SAAS,KAAK,YAAY,IAAI,kBAAA,CAAA,EAAqB,YAAY,IAAI,wBAAA,CAAA,GAA4B;AAC9G,UAAM,eAAe,KAAK,MAAM,MAAM,IAAI;AAC1C,UAAM;EACR;AACF;AAEA,eAAsB,aACpB,UACA,OACA,QACA,gBAA+B;AAE/B,QAAM,WAAW,MAAM,YACrB,MAAM;IAAE;IAAO,MAAM;IAAgB,QAAQ;IAAO;EAAS,CAAA,GAC7D,KAAK,UAAU,MAAA,GACf,cAAA;AAEF,QAAM,OAAO,SAAS;AACtB,MAAI,QAAQ,MAAM;AAChB;EACF;AAEA,QAAM,eAAgB,MAAM,SAAS,KAAI;AACzC,SAAO;AACT;AAEA,eAAe,YACb,KACA,MACA,gBACA,gBAAgB,QAChB,UAAkC,CAAC,GAAC;AAEpC,MAAI;AACJ,MAAI;AACF,eAAW,MAAM,MAAM,KAAK;MAC1B,GAAG,kBAAkB,cAAA;MACrB,QAAQ;MACR,SAAS;QACP,gBAAgB;QAChB,GAAG;MACL;MACA;IACF,CAAA;AACA,QAAI,CAAC,SAAS,IAAI;AAChB,UAAI,UAA8B;AAClC,UAAI;AACF,cAAM,UAAW,MAAM,SAAS,KAAI;AACpC,kBAAU,QAAQ,OAAO;AACzB,YAAI,SAAS,OAAO,SAAS;AAC3B,qBAAW,IAAI,KAAK,UAAU,QAAQ,MAAM,OAAO,CAAA;QACrD;MACF,SAAS,IAAI;MAEb;AACA,YAAM,IAAI,MAAM,IAAI,SAAS,MAAM,IAAI,SAAS,UAAU,KAAK,OAAA,EAAS;IAC1E;EACF,SAASC,IAAG;AACV,YAAQ,IAAIA,EAAA;AACZ,UAAM,MAAM,IAAI,wBAAwB,iCAAiCA,GAAE,OAAO,EAAE;AACpF,QAAI,QAAQA,GAAE;AACd,UAAM;EACR;AACA,SAAO;AACT;AAEA,IAAM,WAAW;AAEjB,SAAS,MAAM,EACb,OACA,MACA,QACA,SAAQ,GAMT;AACC,QAAM,cAAc,MAAM,WAAU;AACpC,QAAM,MAAM,IAAI,IAAI,GAAG,QAAA,IAAY,WAAA,WAAsB,KAAA,IAAS,IAAA,EAAM;AACxE,MAAI,aAAa,OAAO,OAAO,SAAS,MAAM;AAC9C,MAAI,QAAQ;AACV,QAAI,aAAa,OAAO,OAAO,KAAA;EACjC;AACA,SAAO;AACT;AAOA,SAAS,kBAAkB,gBAA+B;AACxD,QAAM,eAAe,CAAC;AACtB,MAAI,gBAAgB,SAAS;AAC3B,UAAM,kBAAkB,IAAI,gBAAA;AAC5B,UAAM,SAAS,gBAAgB;AAC/B,eAAW,MAAM,gBAAgB,MAAK,GAAI,eAAe,OAAO;AAChE,iBAAa,SAAS;EACxB;AACA,SAAO;AACT;;;AC7IO,SAAS,aAAa,UAAiC;AAC5D,MAAI,SAAS,cAAc,SAAS,WAAW,SAAS,GAAG;AACzD,QAAI,SAAS,WAAW,SAAS,GAAG;AAClC,cAAQ,KACN,qBAAqB,SAAS,WAAW,MAAM,6HAA6H;IAEhL;AACA,QAAI,mBAAmB,SAAS,WAAW,CAAA,CAAE,GAAG;AAC9C,YAAM,IAAI,gCACR,GAAG,wBAAwB,QAAA,CAAA,IAC3B,QAAA;IAEJ;AACA,WAAO,QAAQ,QAAA;EACjB;AACA,MAAI,SAAS,gBAAgB;AAC3B,UAAM,IAAI,gCACR,uBAAuB,wBAAwB,QAAA,CAAA,IAC/C,QAAA;EAEJ;AACA,SAAO;AACT;AAKO,SAAS,QAAQ,UAAiC;AACvD,MAAI,SAAS,aAAa,CAAA,EAAG,SAAS,QAAQ,CAAA,GAAI,MAAM;AACtD,WAAO,SAAS,WAAW,CAAA,EAAG,QAAQ,MAAM,CAAA,EAAG;EACjD;AACA,MAAI,SAAS,aAAa,CAAA,EAAG,SAAS,QAAQ,CAAA,GAAI,cAAc;AAC9D,WAAO,SAAS,WAAW,CAAA,EAAG,QAAQ,MAAM,CAAA,EAAG;EACjD;AACA,SAAO;AACT;AAEA,IAAM,mBAAmC;EAAC;EAAc;;AAExD,SAAS,mBAAmB,WAAoB;AAC9C,SAAO,CAAC,CAAC,UAAU,gBAAgB,iBAAiB,SAAS,UAAU,YAAY;AACrF;AAEA,SAAS,wBAAwB,UAAiC;AAChE,MAAI,UAAU;AACd,OAAK,CAAC,SAAS,cAAc,SAAS,WAAW,WAAW,MAAM,SAAS,gBAAgB;AACzF,eAAW;AACX,QAAI,SAAS,gBAAgB,aAAa;AACxC,iBAAW,WAAW,SAAS,eAAe,WAAW;IAC3D;AACA,QAAI,SAAS,gBAAgB,oBAAoB;AAC/C,iBAAW,KAAK,SAAS,eAAe,kBAAkB;IAC5D;EACF,WAAW,SAAS,aAAa,CAAA,GAAI;AACnC,UAAM,iBAAiB,SAAS,WAAW,CAAA;AAC3C,QAAI,mBAAmB,cAAA,GAAiB;AACtC,iBAAW,gCAAgC,eAAe,YAAY;AACtE,UAAI,eAAe,eAAe;AAChC,mBAAW,KAAK,eAAe,aAAa;MAC9C;IACF;EACF;AACA,SAAO;AACT;;;AC/DA,eAAsB,6BACpB,KACA,UACA,KAAY;AAEZ,QAAM,CAAC,OAAO,SAAA,IAAa,SAAS,GAAA;AACpC,MAAI,aAAoC;AAExC,MAAI;AACF,qBAAiB,MAAM,sBAAsB,UAAU,OAAO,SAAA,GAAY;AACxE,YAAM,OAAO,aAAa,EAAA;AAC1B,UAAI,OAAO,SAAS,UAAU;AAC5B,sBAAc;MAChB,OAAO;AACL,qBAAa;AACb;MACF;IACF;EACF,SAAS,KAAK;AAEZ,SAAK,MAAM,GAAA;AACX,SAAK,MAAM,KAAK,WAAW,IAAI,SAAQ,CAAA;AACvC,iBAAa,KAAK,WAAW,IAAI,SAAQ;EAC3C;AAEA,OAAK,MAAM,GAAA;AACX,OAAK,MAAM,UAAA;AAEX,WAAS,cAAc,SAA8B;AACnD,QAAI,OAAO,YAAY,UAAU;AAC/B,aAAO;QACL,IAAI;QACJ,QAAQ;QACR,SAAS,KAAK,MAAM,KAAK,IAAG,IAAK,GAAA;QACjC,OAAO,MAAM;QACb,SAAS;UACP;YACE,SAAS;cAAE,MAAM;cAAa;cAAkB,SAAS;YAAK;YAC9D,eAAe;YACf,OAAO;YACP,UAAU;UACZ;;MAEJ;IACF;AAEA,WAAO;MACL,IAAI;MACJ,QAAQ;MACR,SAAS,KAAK,MAAM,KAAK,IAAG,IAAK,GAAA;MACjC,OAAO,MAAM;MACb,SAAS;QACP;UACE,SAAS;YACP,MAAM;YACN,SAAS;YACT,SAAS;YACT,eAAe;cACb,MAAM,QAAQ,QAAQ;cACtB,WAAW,KAAK,UAAU,QAAQ,IAAI;YACxC;UACF;UACA,eAAe;UACf,OAAO;UACP,UAAU;QACZ;;IAEJ;EACF;AAEA,SAAO,SAAS,KAAK,cAAc,UAAA,CAAA;AACrC;;;ACvEO,SAAS,0BACd,KACA,UACA,KAAY;AAEZ,QAAM,CAAC,OAAO,SAAA,IAAa,SAAS,GAAA;AACpC,OAAK,MAAM,iCAAiC,GAAA;AAC5C,SAAO,YACJ,mBAAA;AACC,QAAI;AACF,uBAAiB,MAAM,sBAAsB,UAAU,OAAO,SAAA,GAAY;AACxE,aAAK,MAAM,8BAA8B,EAAA;AACzC,cAAM,OAAO,aAAa,EAAA;AAC1B,cAAM,cAAc;UAClB,OAAO,MAAM;UACb,SAAS;UACT,MAAM;QACR,CAAA;MACF;IACF,SAAS,OAAO;AACd,YAAM,cAAc;QAClB,OAAO,MAAM;QACb,SAAS,OAAO,WAAW,MAAM,SAAQ;QACzC,MAAM;MACR,CAAA;IACF;AACA,UAAM,cAAc;MAAE,OAAO,MAAM;MAAO,SAAS;MAAI,MAAM;IAAK,CAAA;AAClE,UAAM;AACN,WAAO;EACT,EAAA,CAAA;AAEJ;AAEA,SAAS,cAAc,EACrB,OACA,SACA,KAAI,GAKL;AACC,MAAI,OAAO,YAAY,UAAU;AAC/B,WAAO;MACL,IAAI;MACJ,QAAQ;MACR,SAAS,KAAK,MAAM,KAAK,IAAG,IAAK,GAAA;MACjC;MACA,SAAS;QACP;UACE,OAAO;YAAE,MAAM;YAAa;UAAQ;UACpC,eAAe,OAAO,SAAS;UAC/B,OAAO;QACT;;IAEJ;EACF;AAEA,SAAO;IACL,IAAI;IACJ,QAAQ;IACR,SAAS,KAAK,MAAM,KAAK,IAAG,IAAK,GAAA;IACjC;IACA,SAAS;MACP;QACE,OAAO;UAAE,MAAM;UAAa,eAAe;QAAQ;QACnD,eAAe,OAAO,kBAAkB;QACxC,OAAO;MACT;;EAEJ;AACF;AAEA,IAAM,UAAU,IAAI,YAAA;AAEpB,SAAS,YAAY,YAA+E;AAClG,QAAM,IAAI,IAAI,eAA2B;IACvC,MAAM,KAAK,YAAU;AACnB,YAAM,EAAE,OAAO,KAAI,IAAK,MAAM,WAAW,KAAI;AAC7C,UAAI,MAAM;AACR,mBAAW,MAAK;MAClB,OAAO;AACL,cAAM,OAAO,OAAO,UAAU,WAAW,QAAQ,KAAK,UAAU,KAAA;AAChE,mBAAW,QAAQ,QAAQ,OAAO,SAAS;UAAE;QAAK,CAAA,CAAA,CAAA;MACpD;IACF;EACF,CAAA;AAEA,QAAM,WAAW,IAAI,SAAS,GAAG;IAC/B,QAAQ;IACR,SAAS,IAAI,QAAQ;MACnB,gBAAgB;IAClB,CAAA;EACF,CAAA;AAEA,SAAO;AACT;AAEO,SAAS,SAAS,EAAE,OAAO,MAAM,GAAE,GAAY;AACpD,MAAI,SAAS,SAAS,IAAA;;AACtB,MAAI,OAAO;AACT,cAAU,UAAU,SAAS,EAAA;;EAC/B;AACA,MAAI,IAAI;AACN,cAAU,OAAO,MAAM,EAAA;;EACzB;AACA,SAAO,GAAG,MAAA;;AACZ;;;AC7GA,eAAsB,iBAAiB,QAAe;AACpD,QAAM,MAAO,MAAM,OAAO,KAAI;AAC9B,QAAM,UAAU,OAAO;AACvB,QAAM,WAAW,SAAS,OAAA;AAC1B,MAAI,YAAY,MAAM;AACpB,WAAO,IAAI,SAAS,gBAAgB;MAAE,QAAQ;IAAI,CAAA;EACpD;AAEA,MAAI,IAAI,WAAW,MAAM;AACvB,WAAO,MAAM,6BAA6B,KAAK,UAAU,OAAO,MAAM;EACxE;AACA,SAAO,0BAA0B,KAAK,UAAU,OAAO,MAAM;AAC/D;;;ACZA,eAAsB,sBAAsB,QAAe;AACzD,QAAM,MAAO,MAAM,OAAO,KAAI;AAC9B,QAAM,MAAM,OAAO;AACnB,QAAM,UAAU,OAAO;AACvB,QAAM,WAAW,SAAS,OAAA;AAC1B,MAAI,YAAY,MAAM;AACpB,WAAO,IAAI,SAAS,gBAAgB;MAAE,QAAQ;IAAI,CAAA;EACpD;AAEA,QAAM,sBAA2C;IAC/C,OAAO;IACP,SAAS;MACP,OAAO;QAAC,IAAI;QAAO,KAAI,EAAG,IAAI,CAAC,QAAQ;QAAE,MAAM,GAAG,SAAQ;MAAG,EAAC;IAChE;EACF;AAEA,OAAK,KAAK,WAAW,mBAAA;AAErB,MAAI,aAAmC,CAAA;AAEvC,MAAI;AACF,UAAM,KAAK,MAAM,aAAa,UAAU,IAAI,YAAY,oBAAA,GAAuB,mBAAA;AAC/E,UAAM,OAAO,IAAI,WAAW;AAC5B,iBAAa;EACf,SAAS,KAAK;AAEZ,SAAK,MAAM,GAAA;AACX,SAAK,MAAM,KAAK,WAAW,IAAI,SAAQ,CAAA;AACvC,iBAAa,KAAK,WAAW,IAAI,SAAQ;EAC3C;AAEA,OAAK,MAAM,GAAA;AACX,OAAK,MAAM,UAAA;AAEX,QAAM,OAAkD;IACtD,QAAQ;IACR,MAAM;MACJ;QACE,QAAQ;QACR,OAAO;QACP,WAAW,cAAc,CAAA;MAC3B;;IAEF,OAAO,IAAI;IACX,OAAO;MACL,eAAe;MACf,cAAc;IAChB;EACF;AAEA,SAAO,SAAS,KAAK,IAAA;AACvB;;;ACrDO,IAAM,YAAmC,OAAO,KAAK,YAAA,EAAc,IAAI,CAAC,WAAW;EACxF,SAAS;EACT,QAAQ;EACR,UAAU;EACV,IAAI;AACN,EAAC;AAEM,IAAM,SAAS,OAAO,QAAA;AAC3B,QAAM,WAAW,SAAS,IAAI,OAAO;AACrC,SAAO,MAAM,WAAW,QAAA;AAM1B;AAEO,IAAM,cAAc,CAAC,UAAA;AAC1B,SAAO,UAAU,KAAK,CAAC,OAAO,GAAG,OAAO,KAAA;AAC1C;;;ACZA,IAAM,EAAE,WAAW,QAAO,IAAK,EAAK;EAAE,cAAc;AAAI,CAAA;AAExD,IAAM,MAAM,EAAkC;EAC5C,QAAQ;IACN;IACA,CAAC,QAAA;AACC,UAAI,SAAS,IAAI,OAAO,OAAO,WAAU,EAAG,SAAQ,CAAA;AACpD,UAAI,OAAO,KAAK,OAAO,IAAI,MAAM,IAAI,IAAI,GAAG,EAAE;IAChD;;EAEF,SAAS;IACP;IACA,CAAC,GAAG,QAAA;AACF,UAAI,QAAQ,KAAK,OAAO,IAAI,MAAM,IAAI,IAAI,GAAG,EAAE;IAEjD;;AAEJ,CAAA;AAEA,IAAI,IAAI,KAAK,KAAA;AACb,IAAI,KAAK,wBAAwB,gBAAA;AACjC,IAAI,KAAK,kBAAkB,qBAAA;AAC3B,IAAI,IAAI,cAAc,OAAO,QAAQ,SAAS,KAAK,MAAM,OAAO,GAAA,CAAA,CAAA;AAChE,IAAI,IAAI,qBAAqB,CAAC,MAAM,SAAS,KAAK,YAAY,EAAE,OAAO,KAAK,CAAA,CAAA;AAC5E,IAAI,KAAK,4CAA4C,WAAA;AACrD,IAAI,IAAI,KAAK,MAAM,IAAI,SAAS,kBAAkB;EAAE,QAAQ;AAAI,CAAA,CAAA;;;ACjChE,KAAK,MAAM;EAAE,MAAM;AAAK,GAAG,IAAI,KAAK;",
  "names": ["cors", "options", "e", "Router", "base", "r", "routes", "other", "__proto__", "e"]
}
