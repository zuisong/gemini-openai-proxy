{
  "version": 3,
  "sources": ["../../../../.cache/deno/npm/registry.npmjs.org/itty-router/src/src/cors.ts", "../../../../.cache/deno/npm/registry.npmjs.org/itty-router/src/src/Router.ts", "../src/gemini-proxy.ts", "../src/utils.ts", "../src/hello.ts", "../src/log.ts", "../src/openai/audio/speech/utils.ts", "../src/openai/audio/speech/EdgeProxyHandler.ts", "../src/openai/audio/speech/OaiProxyHandler.ts", "../src/openai/audio/speech/TTSProxyHandler.ts", "../../../../.cache/deno/npm/registry.npmjs.org/eventsource-parser/3.0.1/src/errors.ts", "../../../../.cache/deno/npm/registry.npmjs.org/eventsource-parser/3.0.1/src/parse.ts", "../../../../.cache/deno/npm/registry.npmjs.org/eventsource-parser/3.0.1/src/stream.ts", "../src/gemini-api-client/errors.ts", "../src/gemini-api-client/gemini-api-client.ts", "../src/gemini-api-client/response-helper.ts", "../src/openai/chat/completions/NonStreamingChatProxyHandler.ts", "../src/openai/chat/completions/StreamingChatProxyHandler.ts", "../src/openai/chat/completions/ChatProxyHandler.ts", "../src/openai/embeddingProxyHandler.ts", "../src/openai/models.ts", "../src/app.ts", "../main_bun.ts"],
  "sourcesContent": [null, null, "export async function geminiProxy(rawReq: Request) {\n  const url = new URL(rawReq.url)\n  url.host = \"generativelanguage.googleapis.com\"\n  url.port = \"\"\n  url.protocol = \"https:\"\n  const req = new Request(url, rawReq)\n  const resp = await fetch(req)\n  return new Response(resp.body, resp)\n}\n", "import type { Content, GenerateContentRequest, JsonSchema, Part } from \"./gemini-api-client/types.ts\"\nimport type { Any } from \"./log.ts\"\nimport type { OpenAI } from \"./types.ts\"\n\nexport interface ApiParam {\n  apikey: string\n  useBeta: boolean\n}\n\nexport function getToken(headers: Iterable<[string, string]>): ApiParam | null {\n  for (const [k, v] of headers) {\n    if (k.toLowerCase() !== \"authorization\") continue\n\n    const rawApikey = v.substring(v.indexOf(\" \") + 1)\n\n    if (!rawApikey.includes(\"#\")) {\n      return {\n        apikey: rawApikey,\n        useBeta: false,\n      }\n    }\n\n    // todo read config from apikey\n    const apikey = rawApikey.substring(0, rawApikey.indexOf(\"#\"))\n    const params = new URLSearchParams(rawApikey.substring(rawApikey.indexOf(\"#\") + 1))\n    return {\n      apikey,\n      useBeta: params.has(\"useBeta\"),\n    }\n  }\n  return null\n}\n\nfunction parseBase64(base64: string): Part {\n  if (!base64.startsWith(\"data:\")) {\n    return { text: \"\" }\n  }\n\n  const [m, data, ..._arr] = base64.split(\",\")\n  const mimeType = m.match(/:(?<mime>.*?);/)?.groups?.mime ?? \"img/png\"\n  return {\n    inlineData: {\n      mimeType,\n      data,\n    },\n  }\n}\n\nexport function openAiMessageToGeminiMessage(messages: OpenAI.Chat.ChatCompletionMessageParam[]): Content[] {\n  const result: Content[] = messages.flatMap(({ role, content }) => {\n    if (role === \"system\") {\n      return [\n        {\n          role: \"user\",\n          parts: typeof content !== \"string\" ? content : [{ text: content }],\n        },\n      ] satisfies Content[] as Content[]\n    }\n    const parts: Part[] =\n      content == null || typeof content === \"string\"\n        ? [{ text: content?.toString() ?? \"\" }]\n        : content.map((item) => {\n            if (item.type === \"text\") return { text: item.text }\n            if (item.type === \"image_url\") return parseBase64(item.image_url.url)\n            return { text: \"OK\" }\n          })\n    return [{ role: \"user\" === role ? \"user\" : \"model\", parts: parts }]\n  })\n\n  return result\n}\n\nexport function genModel(req: OpenAI.Chat.ChatCompletionCreateParams): [GeminiModel, GenerateContentRequest] {\n  const model: GeminiModel = GeminiModel.modelMapping(req.model)\n\n  let functions: OpenAI.Chat.FunctionObject[] =\n    req.tools?.filter((it) => it.type === \"function\")?.map((it) => it.function) ?? []\n\n  functions = functions.concat((req.functions ?? []).map((it) => ({ strict: null, ...it })))\n\n  const [responseMimeType, responseSchema] = (() => {\n    switch (req.response_format?.type) {\n      case \"json_object\":\n        return [\"application/json\", undefined]\n      case \"json_schema\":\n        return [\"application/json\", req.response_format.json_schema.schema satisfies JsonSchema | undefined]\n      case \"text\":\n        return [\"text/plain\", undefined]\n      default:\n        return [undefined, undefined]\n    }\n  })()\n\n  const generateContentRequest: GenerateContentRequest = {\n    contents: openAiMessageToGeminiMessage(req.messages),\n    generationConfig: {\n      maxOutputTokens: req.max_completion_tokens ?? undefined,\n      temperature: req.temperature ?? undefined,\n      topP: req.top_p ?? undefined,\n      responseMimeType: responseMimeType,\n      responseSchema: responseSchema,\n      thinkingConfig: !model.isThinkingModel()\n        ? undefined\n        : {\n            includeThoughts: true,\n          },\n    },\n    tools:\n      functions.length === 0\n        ? undefined\n        : [\n            {\n              functionDeclarations: functions,\n            },\n          ],\n    safetySettings: (\n      [\n        \"HARM_CATEGORY_HATE_SPEECH\",\n        \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"HARM_CATEGORY_HARASSMENT\",\n      ] as const\n    ).map((category) => ({\n      category,\n      threshold: \"BLOCK_NONE\",\n    })),\n  }\n  return [model, generateContentRequest]\n}\nexport type KnownGeminiModel =\n  | \"gemini-2.5-pro\"\n  | \"gemini-2.5-flash\"\n  | \"gemini-1.5-pro-latest\"\n  | \"gemini-1.5-flash-latest\"\n  | \"gemini-1.5-flash-8b-latest\"\n  | \"gemini-2.0-pro-exp\"\n  | \"gemini-2.0-flash-exp\"\n  | \"gemma-3-4b-it\"\n  | \"gemma-3-27b-it\"\n  | \"text-embedding-004\"\n  | \"fmtts\"\nconst OAI_GEMINI_MAP_DEALFULT_MODEL = \"gemma-3-4b-it\"\nexport type API_VERSION = \"v1beta\" | \"v1\" | \"v1alpha\"\n\nexport class GeminiModel {\n  static modelMapping(model: string | undefined): GeminiModel {\n    const modelName: GeminiModelName | KnownGeminiModel =\n      ModelMapping[model ?? \"\"] ?? GeminiModel.defaultModel(model ?? \"\")\n    return new GeminiModel(modelName)\n  }\n  public readonly model: GeminiModelName\n  constructor(model: GeminiModelName) {\n    this.model = model\n  }\n\n  isThinkingModel(): boolean {\n    return this.model.includes(\"thinking\")\n  }\n\n  apiVersion(): API_VERSION {\n    if (this.isThinkingModel()) {\n      return \"v1alpha\"\n    }\n    return \"v1beta\"\n  }\n\n  toString(): string {\n    return this.model\n  }\n\n  private static defaultModel(m: string): GeminiModelName {\n    if (m.startsWith(\"gemini\") || m.startsWith(\"gemma\")) {\n      return m as GeminiModelName\n    }\n    return OAI_GEMINI_MAP_DEALFULT_MODEL\n  }\n}\n\nexport type GeminiModelName =\n  | `gemini${string}`\n  | `gemma${string}`\n  | \"text-embedding-004\"\n  | \"embedding-gecko-001\"\n  | \"gemini-embedding-001\"\n  | \"embedding-001\"\n  | \"fmtts\"\n//https://platform.openai.com/docs/guides/embeddings/embedding-models\n//https://ai.google.dev/gemini-api/docs/embeddings\nexport const ModelMapping: Readonly<Record<string, KnownGeminiModel>> = {\n  // Updated with latest models\n  \"gpt-3.5-turbo\": \"gemini-1.5-flash-8b-latest\", // ‚úÖ Good match\n  \"gpt-4o\": \"gemini-2.5-flash\", // Updated to newer model\n  \"gpt-4o-mini\": \"gemini-1.5-flash-8b-latest\", // ‚úÖ Good match\n  \"gpt-4\": \"gemini-2.5-pro\", // Updated to newer model\n  \"gpt-4-vision-preview\": \"gemini-2.5-flash\", // Better multimodal match\n  \"gpt-4-turbo\": \"gemini-2.5-pro\", // Updated to newer model\n  \"gpt-4-turbo-preview\": \"gemini-2.5-pro\", // Better capability match\n  \"gpt-4.1-nano\": \"gemini-1.5-flash-8b-latest\", // More appropriate for smaller model\n  \"gpt-4.1-mini\": \"gemini-2.5-flash\", // Better performance match\n  \"gpt-4.1\": \"gemini-2.5-pro\", // Top-tier match\n  \"gpt-5-nano\": \"gemini-1.5-flash-8b-latest\", // Conservative mapping\n  \"gpt-5-mini\": \"gemini-2.5-flash\", // Performance-focused\n  \"gpt-5\": \"gemini-2.5-pro\", // Best available match\n\n  // Embeddings remain good\n  \"text-embedding-3-small\": \"text-embedding-004\",\n  \"text-embedding-3-large\": \"text-embedding-004\",\n  \"text-embedding-ada-002\": \"text-embedding-004\",\n\n  // TTS mapping\n  \"tts-1\": \"fmtts\", // Keep as is if this works for your use case\n}\n\nexport function getRuntimeKey() {\n  const global = globalThis as typeof globalThis & Record<string, undefined | Any>\n  if (global?.Deno !== undefined) {\n    return \"deno\"\n  }\n  if (global?.Bun !== undefined) {\n    return \"bun\"\n  }\n  if (typeof global?.WebSocketPair === \"function\") {\n    return \"workerd\"\n  }\n  if (typeof global?.EdgeRuntime === \"string\") {\n    return \"edge-light\"\n  }\n  if (global?.fastly !== undefined) {\n    return \"fastly\"\n  }\n  if (global?.process?.release?.name === \"node\") {\n    return \"node\"\n  }\n  return \"other\"\n}\n", "import { getRuntimeKey } from \"./utils.ts\"\n\nexport function hello(req: Request): Response {\n  const origin = new URL(req.url).origin\n  return new Response(`\n    Hello Gemini-OpenAI-Proxy from ${getRuntimeKey()}!\n\n    You can try it with:\n\n    curl ${origin}/v1/chat/completions \\\\\n    -H \"Authorization: Bearer $YOUR_GEMINI_API_KEY\" \\\\\n    -H \"Content-Type: application/json\" \\\\\n    -d '{\n        \"model\": \"gpt-3.5-turbo\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n        \"temperature\": 0.7\n    }'\n    `)\n}\n", "export type Any = Parameters<typeof console.log>[0]\n\nexport interface ILogger {\n  error: (...data: Any[]) => void\n  warn: (...data: Any[]) => void\n  info: (...data: Any[]) => void\n  debug: (...data: Any[]) => void\n}\n\nconst LEVEL = [\"debug\", \"info\", \"warn\", \"error\"] as const\n\ninterface Config {\n  level: (typeof LEVEL)[number]\n  prefix: string\n}\n\nexport class Logger implements ILogger {\n  private config: Config\n\n  debug!: Log\n  info!: Log\n  warn!: Log\n  error!: Log\n\n  constructor(prefix?: string, logLevel?: string) {\n    const level = LEVEL.find((it) => it === logLevel) ?? \"warn\"\n    this.config = {\n      prefix: prefix ?? \"\",\n      level,\n    }\n\n    for (const m of LEVEL) {\n      this[m] = (...data: Any[]) => this.#write(m, ...data)\n    }\n  }\n\n  #write(level: Config[\"level\"], ...data: Any[]) {\n    const { level: configLevel, prefix } = this.config\n    if (LEVEL.indexOf(level) < LEVEL.indexOf(configLevel)) {\n      return\n    }\n\n    console[level](`${new Date().toISOString()} ${level.toUpperCase()}${prefix ? ` ${prefix}` : \"\"}`, ...data)\n  }\n}\n\ntype Log = typeof console.log\n", "export interface TTSParam {\n  model: string\n  input: string\n  voice: string\n  response_format?: string\n  instructions?: string\n}\n\nasync function hmacSha256(key: Uint8Array, data: string): Promise<Uint8Array> {\n  const cryptoKey = await crypto.subtle.importKey(\"raw\", key, { name: \"HMAC\", hash: { name: \"SHA-256\" } }, false, [\n    \"sign\",\n  ])\n  const signature = await crypto.subtle.sign(\"HMAC\", cryptoKey, new TextEncoder().encode(data))\n  return new Uint8Array(signature)\n}\n\nasync function base64ToBytes(base64: string): Promise<Uint8Array> {\n  const binaryString = atob(base64)\n  const bytes = new Uint8Array(binaryString.length)\n  for (let i = 0; i < binaryString.length; i++) {\n    bytes[i] = binaryString.charCodeAt(i)\n  }\n  return bytes\n}\n\nasync function bytesToBase64(bytes: Uint8Array): Promise<string> {\n  return btoa(String.fromCharCode.apply(null, Array.from(bytes)))\n}\n\nfunction uuid(): string {\n  return crypto.randomUUID().replace(/-/g, \"\")\n}\n\nexport async function sign(urlStr: string): Promise<string> {\n  const url = urlStr.split(\"://\")[1]\n  const encodedUrl = encodeURIComponent(url)\n  const uuidStr = uuid()\n  const formattedDate = dateFormat()\n  const bytesToSign = `MSTranslatorAndroidApp${encodedUrl}${formattedDate}${uuidStr}`.toLowerCase()\n  const decode = await base64ToBytes(\n    \"oik6PdDdMnOXemTbwvMn9de/h9lFnfBaCWbGMMZqqoSaQaqUOqjVGm5NqsmjcBI1x+sS9ugjB55HEJWRiFXYFw==\",\n  )\n  const signData = await hmacSha256(decode, bytesToSign)\n  const signBase64 = await bytesToBase64(signData)\n  return `MSTranslatorAndroidApp::${signBase64}::${formattedDate}::${uuidStr}`\n}\n\nfunction dateFormat(): string {\n  const formattedDate = new Date().toUTCString().replace(/GMT/, \"\").trim() + \" GMT\"\n  return formattedDate.toLowerCase()\n}\n\nexport function splitAndMerge(str: string, chunkSize: number, determin) {\n  const sentences = str.split(determin)\n  const result: Array<string> = []\n  let currentChunk: Array<string> = []\n  let currentLength = 0\n\n  for (const sentence of sentences) {\n    if (currentLength + sentence.length + (currentChunk.length > 0 ? 1 : 0) <= chunkSize) {\n      currentChunk.push(sentence)\n      currentLength += sentence.length + (currentChunk.length > 1 ? 1 : 0)\n    } else {\n      if (currentChunk.length > 0) {\n        result.push(currentChunk.join(\" \"))\n      }\n      currentChunk = [sentence]\n      currentLength = sentence.length\n    }\n  }\n\n  if (currentChunk.length > 0) {\n    result.push(currentChunk.join(\" \"))\n  }\n\n  return result\n}\n\nexport function makeCORSHeaders() {\n  return {\n    \"Access-Control-Allow-Origin\": \"*\",\n    \"Access-Control-Allow-Methods\": \"GET,HEAD,POST,OPTIONS\",\n    \"Access-Control-Allow-Headers\": \"Content-Type, x-api-key\",\n    \"Access-Control-Max-Age\": \"86400\",\n  }\n}\n", "import { makeCORSHeaders, sign, splitAndMerge, type TTSParam } from \"./utils.ts\"\n\nconst DEFAULT_AUDIO_FORMAT = \"audio-24khz-48kbitrate-mono-mp3\"\nconst Edge_TTS_VOICE_URL =\n  \"https://speech.platform.bing.com/consumer/speech/synthesize/readaloud/voices/list?trustedclienttoken=6A5AA1D4EAFF4E9FB37E23D68491D6F4\"\n\nlet VOICE_LIST: string[] = []\nconst TOKEN_REFRESH_BEFORE_EXPIRY = 3 * 60\nlet tokenInfo: {\n  endpoint: EdgeEndpointParam | null\n  token: string | null\n  expiredAt: number | null\n} = {\n  endpoint: null,\n  token: null,\n  expiredAt: null,\n}\n\nlet Edge_TTS_ENDPOINT_URL = \"\"\nlet EDGE_ENDPONT2: EdgeEndpointParam | null = null\ninterface EdgeEndpointParam {\n  t: string\n  r: string\n}\n\ninterface EdgeEndpointJWTParam {\n  exp: number\n}\n\nasync function getVoice() {\n  const EDGE_ENDPONT = await getEndpoint()\n  const response = await fetch(Edge_TTS_VOICE_URL, {\n    method: \"GET\",\n    headers: {\n      Authorization: EDGE_ENDPONT?.t ?? \"\",\n      \"Content-Type\": \"application/ssml+xml\",\n      \"User-Agent\":\n        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36 Edg/127.0.0.0\",\n      \"X-Microsoft-OutputFormat\": DEFAULT_AUDIO_FORMAT,\n    },\n  })\n\n  const data = await response.json()\n  VOICE_LIST = data.map((v: any) => v[\"ShortName\"])\n  const VOICE_LIST_en = VOICE_LIST.filter((x) => x.startsWith(\"en\"))\n  const VOICE_LIST_zh = VOICE_LIST.filter((x) => x.startsWith(\"zh\"))\n  const VOICE_LIST_ja = VOICE_LIST.filter((x) => x.startsWith(\"ja\"))\n  VOICE_LIST = VOICE_LIST_en.concat(VOICE_LIST_zh.concat(VOICE_LIST_ja))\n  console.error(VOICE_LIST)\n}\n\nasync function getEndpoint(): Promise<EdgeEndpointParam> {\n  if (EDGE_ENDPONT2) {\n    return EDGE_ENDPONT2\n  }\n  EDGE_ENDPONT2 = await getEndpoint2()\n\n  return EDGE_ENDPONT2\n}\n\nasync function getEndpoint2(): Promise<EdgeEndpointParam> {\n  const now = Date.now() / 1000\n\n  if (tokenInfo.token && tokenInfo.expiredAt && now < tokenInfo.expiredAt - TOKEN_REFRESH_BEFORE_EXPIRY) {\n    return tokenInfo.endpoint!\n  }\n\n  // Ëé∑ÂèñÊñ∞token\n  const endpointUrl = \"https://dev.microsofttranslator.com/apps/endpoint?api-version=1.0\"\n  const clientId = crypto.randomUUID().replace(/-/g, \"\")\n\n  try {\n    const response = await fetch(endpointUrl, {\n      method: \"POST\",\n      headers: {\n        \"Accept-Language\": \"zh-Hans\",\n        \"X-ClientVersion\": \"4.0.530a 5fe1dc6c\",\n        \"X-UserId\": \"0f04d16a175c411e\",\n        \"X-HomeGeographicRegion\": \"zh-Hans-CN\",\n        \"X-ClientTraceId\": clientId,\n        \"X-MT-Signature\": await sign(endpointUrl),\n        \"User-Agent\":\n          \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36 Edg/127.0.0.0\",\n        \"Content-Type\": \"application/json; charset=utf-8\",\n        \"Content-Length\": \"0\",\n        \"Accept-Encoding\": \"gzip\",\n      },\n    })\n\n    if (!response.ok) {\n      throw new Error(`Ëé∑ÂèñendpointÂ§±Ë¥•: ${response.status}`)\n    }\n\n    const data = (await response.json()) as EdgeEndpointParam\n    const jwt = data.t.split(\".\")[1]\n    const decodedJwt = JSON.parse(atob(jwt)) as EdgeEndpointJWTParam\n\n    tokenInfo = {\n      endpoint: data,\n      token: data.t,\n      expiredAt: decodedJwt.exp,\n    }\n\n    return data\n  } catch (error) {\n    console.error(\"Ëé∑ÂèñendpointÂ§±Ë¥•:\", error)\n    // Â¶ÇÊûúÊúâÁºìÂ≠òÁöÑtokenÔºåÂç≥‰ΩøËøáÊúü‰πüÂ∞ùËØï‰ΩøÁî®\n    if (tokenInfo.token) {\n      console.log(\"‰ΩøÁî®ËøáÊúüÁöÑÁºìÂ≠òtoken\")\n      return tokenInfo.endpoint!\n    }\n    throw error\n  }\n}\n\nfunction getSsml(\n  text: string,\n  voiceName: string,\n  rate?: string,\n  pitch?: string,\n  volume?: string,\n  style?: string,\n  slien = 0,\n) {\n  let slien_str = \"\"\n  if (slien > 0) {\n    slien_str = `<break time=\"${slien}ms\" />`\n  }\n  return `<speak xmlns=\"http://www.w3.org/2001/10/synthesis\" xmlns:mstts=\"http://www.w3.org/2001/mstts\" version=\"1.0\" xml:lang=\"zh-CN\"> \n                <voice name=\"${voiceName}\"> \n                    <mstts:express-as style=\"${style}\"  styledegree=\"2.0\" role=\"default\" > \n                        <prosody rate=\"${rate}\" pitch=\"${pitch}\" volume=\"${volume}\">${text}</prosody> \n                    </mstts:express-as> \n                    ${slien_str}\n                </voice> \n            </speak>`\n}\n\ninterface EdgeRequestParam {\n  text: string\n  voiceName: string\n  rate?: string\n  pitch?: string\n  volume?: string\n  style?: string\n  slien?: number\n}\n\nasync function EdgeProxyDownloader(formData: EdgeRequestParam): Promise<Response> {\n  const EDGE_ENDPONT = await getEndpoint()\n  try {\n    const response = await fetch(Edge_TTS_ENDPOINT_URL, {\n      method: \"POST\",\n      headers: {\n        Authorization: EDGE_ENDPONT?.t ?? \"\",\n        \"Content-Type\": \"application/ssml+xml\",\n        \"User-Agent\":\n          \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36 Edg/127.0.0.0\",\n        \"X-Microsoft-OutputFormat\": DEFAULT_AUDIO_FORMAT,\n      },\n      body: getSsml(\n        formData.text,\n        formData.voiceName,\n        formData.rate,\n        formData.pitch,\n        formData.volume,\n        formData.style,\n        formData.slien,\n      ),\n    })\n\n    if (!response.ok) {\n      const errorText = await response.text()\n      console.error(\"TTS API Error:\", {\n        status: response.status,\n        statusText: response.statusText,\n        body: errorText,\n        headers: Object.fromEntries(response.headers.entries()),\n      })\n      console.error(\"TTS formData:\", formData)\n      console.error(\"TTS DEFAULT_AUDIO_FORMAT:\", DEFAULT_AUDIO_FORMAT)\n\n      return new Response(\n        JSON.stringify({\n          error: `TTS API error: ${response.status} ${response.statusText}`,\n          details: errorText,\n        }),\n        {\n          status: response.status,\n          headers: { \"Content-Type\": \"application/json\" },\n        },\n      )\n    }\n\n    // Return the audio response\n    return new Response(response.body, {\n      status: 200,\n      headers: {\n        \"Content-Type\": response.headers.get(\"Content-Type\") || \"audio/mpeg\",\n        \"Content-Length\": response.headers.get(\"Content-Length\") || \"\",\n      },\n    })\n  } catch (error) {\n    console.error(\"TTS Handler Error:\", error)\n    return new Response(\n      JSON.stringify({\n        error: \"Internal server error\",\n        message: error instanceof Error ? error.message : \"Unknown error\",\n      }),\n      {\n        status: 500,\n        headers: { \"Content-Type\": \"application/json\" },\n      },\n    )\n  }\n}\n\nexport async function EdgeProxyHandler(req: TTSParam): Promise<Response> {\n  const EDGE_ENDPONT = await getEndpoint()\n  Edge_TTS_ENDPOINT_URL = `https://${EDGE_ENDPONT?.r}.tts.speech.microsoft.com/cognitiveservices/v1`\n\n  if (VOICE_LIST.length === 0) {\n    await getVoice()\n  }\n\n  const maxChunkSize = 4096\n  const chunks = splitAndMerge(req.input.trim(), maxChunkSize, \"\\n\")\n  const audioChunks: Array<Blob> = []\n  while (chunks.length > 0) {\n    try {\n      // const ClientId = crypto.randomUUID().replace(/-/g, \"\");\n      // const generation = crypto.randomUUID().toString();\n\n      let voice = req.voice\n      if (!VOICE_LIST.includes(voice)) {\n        voice = VOICE_LIST[0]\n      }\n\n      // Create form data as URLSearchParams (application/x-www-form-urlencoded)\n      const formData = {\n        text: chunks.shift(),\n        voiceName: voice,\n        rate: \"+0%\",\n        pitch: \"+0Hz\",\n        volume: \"+0%\",\n        style: `general`,\n        slien: 0,\n      }\n      const audio_chunk = await (await EdgeProxyDownloader(formData as EdgeRequestParam)).blob()\n\n      audioChunks.push(audio_chunk)\n    } catch (error) {\n      console.error(\"TTS Handler Error:\", error)\n      return new Response(\n        JSON.stringify({\n          error: \"Internal server error\",\n          message: error instanceof Error ? error.message : \"Unknown error\",\n        }),\n        {\n          status: 500,\n          headers: { \"Content-Type\": \"application/json\" },\n        },\n      )\n    }\n  }\n\n  const concatenatedAudio = new Blob(audioChunks, { type: \"audio/mpeg\" })\n  const response = new Response(concatenatedAudio, {\n    headers: {\n      \"Content-Type\": \"audio/mpeg\",\n      ...makeCORSHeaders(),\n    },\n  })\n\n  return response\n}\n", "import { makeCORSHeaders, splitAndMerge, type TTSParam } from \"./utils.ts\"\n\nconst OAI_TTS_ENDPOINT_URL = \"https://www.openai.fm/api/generate\"\nconst DEFAULT_AUDIO_FORMAT = \"mp3\"\n\n//https://platform.openai.com/docs/guides/text-to-speech\n\nconst VOICE_LIST = [\"alloy\", \"ash\", \"ballad\", \"coral\", \"echo\", \"fable\", \"nova\", \"onyx\", \"sage\", \"shimmer\"]\n\nconst DEFAULT_PROMPT = `Voice Affect: Calm, composed, and reassuring; project quiet authority and confidence, BBC reporter host accent.\nTone: Sincere, empathetic, and gently authoritative‚Äîexpress genuine apology while conveying competence.\nPacing: Steady and moderate; unhurried enough to communicate care, yet efficient enough to demonstrate professionalism.\nEmotion: Genuine empathy and understanding; speak with warmth, especially during apologies (\"I'm very sorry for any disruption...\").\nPronunciation: Clear and precise, emphasizing key reassurances (\"smoothly,\" \"quickly,\" \"promptly\") to reinforce confidence.\nPauses: Brief pauses after offering assistance or requesting details, highlighting willingness to listen and support.`\n\nexport async function OaiProxyDownloader(formData: URLSearchParams): Promise<Response> {\n  try {\n    // const ClientId = crypto.randomUUID().replace(/-/g, \"\");\n\n    // For POST, we need to sign the URL + the form data or just the URL\n    // const Signature = await sign(OAI_TTS_ENDPOINT_URL);\n\n    const response = await fetch(OAI_TTS_ENDPOINT_URL, {\n      method: \"POST\",\n      headers: {\n        // \"X-ClientVersion\": \"4.0.530a 5fe1dc6c\",\n        // \"X-UserId\": \"0f04d16a175c411e\",\n        // \"X-ClientTraceId\": ClientId,\n        // \"X-MT-Signature\": Signature,\n        // \"X-HomeGeographicRegion\": \"en-US\",\n        \"User-Agent\":\n          \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36 Edg/127.0.0.0\",\n        // KEY CHANGE: Use form content type instead of JSON\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n        Accept: \"audio/*\",\n        \"Accept-Encoding\": \"gzip, deflate, br\",\n      },\n      // KEY CHANGE: Send form data as string, not JSON\n      body: formData.toString(),\n    })\n\n    if (!response.ok) {\n      const errorText = await response.text()\n      console.error(\"TTS API Error:\", {\n        status: response.status,\n        statusText: response.statusText,\n        body: errorText,\n        headers: Object.fromEntries(response.headers.entries()),\n      })\n      console.error(\"TTS formData:\", formData)\n\n      return new Response(\n        JSON.stringify({\n          error: `TTS API error: ${response.status} ${response.statusText}`,\n          details: errorText,\n        }),\n        {\n          status: response.status,\n          headers: { \"Content-Type\": \"application/json\" },\n        },\n      )\n    }\n\n    // Return the audio response\n    return new Response(response.body, {\n      status: 200,\n      headers: {\n        \"Content-Type\": response.headers.get(\"Content-Type\") || \"audio/mpeg\",\n        \"Content-Length\": response.headers.get(\"Content-Length\") || \"\",\n      },\n    })\n  } catch (error) {\n    console.error(\"TTS Handler Error:\", error)\n    return new Response(\n      JSON.stringify({\n        error: \"Internal server error\",\n        message: error instanceof Error ? error.message : \"Unknown error\",\n      }),\n      {\n        status: 500,\n        headers: { \"Content-Type\": \"application/json\" },\n      },\n    )\n  }\n}\nexport async function OaiProxyHandler(req: TTSParam): Promise<Response> {\n  const maxChunkSize = 4096\n  const chunks = splitAndMerge(req.input.trim(), maxChunkSize, \"\\n\")\n  const audioChunks: Array<Blob> = []\n  while (chunks.length > 0) {\n    try {\n      // const ClientId = crypto.randomUUID().replace(/-/g, \"\");\n      const generation = crypto.randomUUID().toString()\n\n      // Create form data as URLSearchParams (application/x-www-form-urlencoded)\n      let voice = req.voice\n      if (!VOICE_LIST.includes(voice)) {\n        voice = VOICE_LIST[0]\n      }\n      const formData = new URLSearchParams({\n        input: chunks.shift(),\n        voice: voice,\n        generation: generation,\n        response_format: req.response_format ?? DEFAULT_AUDIO_FORMAT,\n        prompt: req.instructions ?? DEFAULT_PROMPT,\n      })\n      const audio_chunk = await (await OaiProxyDownloader(formData)).blob()\n\n      audioChunks.push(audio_chunk)\n    } catch (error) {\n      console.error(\"TTS Handler Error:\", error)\n      return new Response(\n        JSON.stringify({\n          error: \"Internal server error\",\n          message: error instanceof Error ? error.message : \"Unknown error\",\n        }),\n        {\n          status: 500,\n          headers: { \"Content-Type\": \"application/json\" },\n        },\n      )\n    }\n  }\n\n  const concatenatedAudio = new Blob(audioChunks, { type: \"audio/mpeg\" })\n  const response = new Response(concatenatedAudio, {\n    headers: {\n      \"Content-Type\": \"audio/mpeg\",\n      ...makeCORSHeaders(),\n    },\n  })\n\n  return response\n}\n", "import { getToken } from \"../../../utils.ts\"\nimport { EdgeProxyHandler } from \"./EdgeProxyHandler.ts\"\nimport { OaiProxyHandler } from \"./OaiProxyHandler.ts\"\nimport type { TTSParam } from \"./utils.ts\"\n\nexport async function ttsProxyHandler(rawReq: Request): Promise<Response> {\n  const req = (await rawReq.json()) as TTSParam\n  const headers = rawReq.headers\n  const apiParam = getToken(headers)\n\n  if (apiParam == null) {\n    return new Response(\"Unauthorized\", { status: 401 })\n  }\n\n  if (req.model === \"tts-1\") {\n    return OaiProxyHandler(req)\n  }\n  return EdgeProxyHandler(req)\n}\n", "/**\n * The type of error that occurred.\n * @public\n */\nexport type ErrorType = 'invalid-retry' | 'unknown-field'\n\n/**\n * Error thrown when encountering an issue during parsing.\n *\n * @public\n */\nexport class ParseError extends Error {\n  /**\n   * The type of error that occurred.\n   */\n  type: ErrorType\n\n  /**\n   * In the case of an unknown field encountered in the stream, this will be the field name.\n   */\n  field?: string\n\n  /**\n   * In the case of an unknown field encountered in the stream, this will be the value of the field.\n   */\n  value?: string\n\n  /**\n   * The line that caused the error, if available.\n   */\n  line?: string\n\n  constructor(\n    message: string,\n    options: {type: ErrorType; field?: string; value?: string; line?: string},\n  ) {\n    super(message)\n    this.name = 'ParseError'\n    this.type = options.type\n    this.field = options.field\n    this.value = options.value\n    this.line = options.line\n  }\n}\n", "/**\n * EventSource/Server-Sent Events parser\n * @see https://html.spec.whatwg.org/multipage/server-sent-events.html\n */\nimport {ParseError} from './errors.ts'\nimport type {EventSourceParser, ParserCallbacks} from './types.ts'\n\n// eslint-disable-next-line @typescript-eslint/no-unused-vars\nfunction noop(_arg: unknown) {\n  // intentional noop\n}\n\n/**\n * Creates a new EventSource parser.\n *\n * @param callbacks - Callbacks to invoke on different parsing events:\n *   - `onEvent` when a new event is parsed\n *   - `onError` when an error occurs\n *   - `onRetry` when a new reconnection interval has been sent from the server\n *   - `onComment` when a comment is encountered in the stream\n *\n * @returns A new EventSource parser, with `parse` and `reset` methods.\n * @public\n */\nexport function createParser(callbacks: ParserCallbacks): EventSourceParser {\n  if (typeof callbacks === 'function') {\n    throw new TypeError(\n      '`callbacks` must be an object, got a function instead. Did you mean `{onEvent: fn}`?',\n    )\n  }\n\n  const {onEvent = noop, onError = noop, onRetry = noop, onComment} = callbacks\n\n  let incompleteLine = ''\n\n  let isFirstChunk = true\n  let id: string | undefined\n  let data = ''\n  let eventType = ''\n\n  function feed(newChunk: string) {\n    // Strip any UTF8 byte order mark (BOM) at the start of the stream\n    const chunk = isFirstChunk ? newChunk.replace(/^\\xEF\\xBB\\xBF/, '') : newChunk\n\n    // If there was a previous incomplete line, append it to the new chunk,\n    // so we may process it together as a new (hopefully complete) chunk.\n    const [complete, incomplete] = splitLines(`${incompleteLine}${chunk}`)\n\n    for (const line of complete) {\n      parseLine(line)\n    }\n\n    incompleteLine = incomplete\n    isFirstChunk = false\n  }\n\n  function parseLine(line: string) {\n    // If the line is empty (a blank line), dispatch the event\n    if (line === '') {\n      dispatchEvent()\n      return\n    }\n\n    // If the line starts with a U+003A COLON character (:), ignore the line.\n    if (line.startsWith(':')) {\n      if (onComment) {\n        onComment(line.slice(line.startsWith(': ') ? 2 : 1))\n      }\n      return\n    }\n\n    // If the line contains a U+003A COLON character (:)\n    const fieldSeparatorIndex = line.indexOf(':')\n    if (fieldSeparatorIndex !== -1) {\n      // Collect the characters on the line before the first U+003A COLON character (:),\n      // and let `field` be that string.\n      const field = line.slice(0, fieldSeparatorIndex)\n\n      // Collect the characters on the line after the first U+003A COLON character (:),\n      // and let `value` be that string. If value starts with a U+0020 SPACE character,\n      // remove it from value.\n      const offset = line[fieldSeparatorIndex + 1] === ' ' ? 2 : 1\n      const value = line.slice(fieldSeparatorIndex + offset)\n\n      processField(field, value, line)\n      return\n    }\n\n    // Otherwise, the string is not empty but does not contain a U+003A COLON character (:)\n    // Process the field using the whole line as the field name, and an empty string as the field value.\n    // üëÜ This is according to spec. That means that a line that has the value `data` will result in\n    // a newline being added to the current `data` buffer, for instance.\n    processField(line, '', line)\n  }\n\n  function processField(field: string, value: string, line: string) {\n    // Field names must be compared literally, with no case folding performed.\n    switch (field) {\n      case 'event':\n        // Set the `event type` buffer to field value\n        eventType = value\n        break\n      case 'data':\n        // Append the field value to the `data` buffer, then append a single U+000A LINE FEED(LF)\n        // character to the `data` buffer.\n        data = `${data}${value}\\n`\n        break\n      case 'id':\n        // If the field value does not contain U+0000 NULL, then set the `ID` buffer to\n        // the field value. Otherwise, ignore the field.\n        id = value.includes('\\0') ? undefined : value\n        break\n      case 'retry':\n        // If the field value consists of only ASCII digits, then interpret the field value as an\n        // integer in base ten, and set the event stream's reconnection time to that integer.\n        // Otherwise, ignore the field.\n        if (/^\\d+$/.test(value)) {\n          onRetry(parseInt(value, 10))\n        } else {\n          onError(\n            new ParseError(`Invalid \\`retry\\` value: \"${value}\"`, {\n              type: 'invalid-retry',\n              value,\n              line,\n            }),\n          )\n        }\n        break\n      default:\n        // Otherwise, the field is ignored.\n        onError(\n          new ParseError(\n            `Unknown field \"${field.length > 20 ? `${field.slice(0, 20)}‚Ä¶` : field}\"`,\n            {type: 'unknown-field', field, value, line},\n          ),\n        )\n        break\n    }\n  }\n\n  function dispatchEvent() {\n    const shouldDispatch = data.length > 0\n    if (shouldDispatch) {\n      onEvent({\n        id,\n        event: eventType || undefined,\n        // If the data buffer's last character is a U+000A LINE FEED (LF) character,\n        // then remove the last character from the data buffer.\n        data: data.endsWith('\\n') ? data.slice(0, -1) : data,\n      })\n    }\n\n    // Reset for the next event\n    id = undefined\n    data = ''\n    eventType = ''\n  }\n\n  function reset(options: {consume?: boolean} = {}) {\n    if (incompleteLine && options.consume) {\n      parseLine(incompleteLine)\n    }\n\n    isFirstChunk = true\n    id = undefined\n    data = ''\n    eventType = ''\n    incompleteLine = ''\n  }\n\n  return {feed, reset}\n}\n\n/**\n * For the given `chunk`, split it into lines according to spec, and return any remaining incomplete line.\n *\n * @param chunk - The chunk to split into lines\n * @returns A tuple containing an array of complete lines, and any remaining incomplete line\n * @internal\n */\nfunction splitLines(chunk: string): [complete: Array<string>, incomplete: string] {\n  /**\n   * According to the spec, a line is terminated by either:\n   * - U+000D CARRIAGE RETURN U+000A LINE FEED (CRLF) character pair\n   * - a single U+000A LINE FEED(LF) character not preceded by a U+000D CARRIAGE RETURN(CR) character\n   * - a single U+000D CARRIAGE RETURN(CR) character not followed by a U+000A LINE FEED(LF) character\n   */\n  const lines: Array<string> = []\n  let incompleteLine = ''\n  let searchIndex = 0\n\n  while (searchIndex < chunk.length) {\n    // Find next line terminator\n    const crIndex = chunk.indexOf('\\r', searchIndex)\n    const lfIndex = chunk.indexOf('\\n', searchIndex)\n\n    // Determine line end\n    let lineEnd = -1\n    if (crIndex !== -1 && lfIndex !== -1) {\n      // CRLF case\n      lineEnd = Math.min(crIndex, lfIndex)\n    } else if (crIndex !== -1) {\n      lineEnd = crIndex\n    } else if (lfIndex !== -1) {\n      lineEnd = lfIndex\n    }\n\n    // Extract line if terminator found\n    if (lineEnd === -1) {\n      // No terminator found, rest is incomplete\n      incompleteLine = chunk.slice(searchIndex)\n      break\n    } else {\n      const line = chunk.slice(searchIndex, lineEnd)\n      lines.push(line)\n\n      // Move past line terminator\n      searchIndex = lineEnd + 1\n      if (chunk[searchIndex - 1] === '\\r' && chunk[searchIndex] === '\\n') {\n        searchIndex++\n      }\n    }\n  }\n\n  return [lines, incompleteLine]\n}\n", "import {createParser} from './parse.ts'\nimport type {EventSourceMessage, EventSourceParser} from './types.ts'\n\n/**\n * Options for the EventSourceParserStream.\n *\n * @public\n */\nexport interface StreamOptions {\n  /**\n   * Behavior when a parsing error occurs.\n   *\n   * - A custom function can be provided to handle the error.\n   * - `'terminate'` will error the stream and stop parsing.\n   * - Any other value will ignore the error and continue parsing.\n   *\n   * @defaultValue `undefined`\n   */\n  onError?: 'terminate' | ((error: Error) => void)\n\n  /**\n   * Callback for when a reconnection interval is sent from the server.\n   *\n   * @param retry - The number of milliseconds to wait before reconnecting.\n   */\n  onRetry?: (retry: number) => void\n\n  /**\n   * Callback for when a comment is encountered in the stream.\n   *\n   * @param comment - The comment encountered in the stream.\n   */\n  onComment?: (comment: string) => void\n}\n\n/**\n * A TransformStream that ingests a stream of strings and produces a stream of `EventSourceMessage`.\n *\n * @example Basic usage\n * ```\n * const eventStream =\n *   response.body\n *     .pipeThrough(new TextDecoderStream())\n *     .pipeThrough(new EventSourceParserStream())\n * ```\n *\n * @example Terminate stream on parsing errors\n * ```\n * const eventStream =\n *  response.body\n *   .pipeThrough(new TextDecoderStream())\n *   .pipeThrough(new EventSourceParserStream({terminateOnError: true}))\n * ```\n *\n * @public\n */\nexport class EventSourceParserStream extends TransformStream<string, EventSourceMessage> {\n  constructor({onError, onRetry, onComment}: StreamOptions = {}) {\n    let parser!: EventSourceParser\n\n    super({\n      start(controller) {\n        parser = createParser({\n          onEvent: (event) => {\n            controller.enqueue(event)\n          },\n          onError(error) {\n            if (onError === 'terminate') {\n              controller.error(error)\n            } else if (typeof onError === 'function') {\n              onError(error)\n            }\n\n            // Ignore by default\n          },\n          onRetry,\n          onComment,\n        })\n      },\n      transform(chunk) {\n        parser.feed(chunk)\n      },\n    })\n  }\n}\n\nexport {type ErrorType, ParseError} from './errors.ts'\nexport type {EventSourceMessage} from './types.ts'\n", "export class GoogleGenerativeAIError extends Error {\n  constructor(message: string) {\n    super(`[GoogleGenerativeAI Error]: ${message}`)\n  }\n}\n\nexport class GoogleGenerativeAIResponseError<T> extends GoogleGenerativeAIError {\n  public response?: T\n  constructor(message: string, response?: T) {\n    super(message)\n    this.response = response\n  }\n}\n", "import { EventSourceParserStream } from \"eventsource-parser/stream\"\nimport type { components } from \"../generated-types/gemini-types.ts\"\nimport type { ApiParam, GeminiModel } from \"../utils.ts\"\nimport { GoogleGenerativeAIError } from \"./errors.ts\"\nimport type {\n  EmbedContentRequest,\n  EmbedContentResponse,\n  GenerateContentRequest,\n  GenerateContentResponse,\n  RequestOptions,\n} from \"./types.ts\"\n\n// +++ NEW +++\n// Define the request structure for batchEmbedContents\n// It's an array of individual embedContent requests.\nexport interface BatchEmbedContentsRequest {\n  requests: EmbedContentRequest[]\n}\n\n// +++ NEW +++\n// Define the response structure for batchEmbedContents\n// It returns a list of embedding objects.\nexport interface BatchEmbedContentsResponse {\n  embeddings: {\n    values: number[]\n    // The schema might have other fields like `model`, `statistics`, etc.\n    // You would have to add them here manually.\n  }[]\n}\n\ninterface Task {\n  streamGenerateContent: {\n    request: GenerateContentRequest\n    response: GenerateContentResponse\n  }\n  embedContent: {\n    request: EmbedContentRequest\n    response: EmbedContentResponse\n  }\n  // +++ NEW +++ Add the new task to our interface\n  batchEmbedContents: {\n    request: BatchEmbedContentsRequest\n    response: BatchEmbedContentsResponse\n  }\n}\n\nexport async function listModels(apiParam: ApiParam | null) {\n  const url = new URL(`${BASE_URL}/v1beta/openai/models`)\n  const resp = await makeRequest(url, undefined, undefined, \"GET\", {\n    Authorization: `Bearer ${apiParam?.apikey ?? \"\"}`,\n  })\n  return (await resp.json()) as components[\"schemas\"][\"ListModelsResponse\"]\n}\nexport async function* streamGenerateContent(\n  apiParam: ApiParam,\n  model: GeminiModel,\n  params: Task[\"streamGenerateContent\"][\"request\"],\n  requestOptions?: RequestOptions,\n) {\n  const response = await makeRequest(\n    toURL({ model, task: \"streamGenerateContent\", stream: true, apiParam }),\n    JSON.stringify(params),\n    requestOptions,\n  )\n  const body = response.body\n  if (body == null) {\n    return\n  }\n\n  for await (const event of body.pipeThrough(new TextDecoderStream()).pipeThrough(new EventSourceParserStream())) {\n    const responseJson = JSON.parse(event.data) as Task[\"streamGenerateContent\"][\"response\"]\n    yield responseJson\n  }\n}\n\nexport async function embedContent(\n  apiParam: ApiParam,\n  model: GeminiModel,\n  params: Task[\"embedContent\"][\"request\"],\n  requestOptions?: RequestOptions,\n) {\n  const response = await makeRequest(\n    toURL({ model, task: \"embedContent\", stream: false, apiParam }),\n    JSON.stringify(params),\n    requestOptions,\n  )\n  const body = response.body\n  if (body == null) {\n    return\n  }\n\n  const responseJson = (await response.json()) as Task[\"embedContent\"][\"response\"]\n  return responseJson\n}\n\n// +++ NEW +++\n/**\n * Makes a batch embedding request.\n */\nexport async function batchEmbedContents(\n  apiParam: ApiParam,\n  model: GeminiModel,\n  params: Task[\"batchEmbedContents\"][\"request\"],\n  requestOptions?: RequestOptions,\n): Promise<Task[\"batchEmbedContents\"][\"response\"]> {\n  const response = await makeRequest(\n    toURL({ model, task: \"batchEmbedContents\", stream: false, apiParam }),\n    JSON.stringify(params),\n    requestOptions,\n  )\n\n  const responseJson = (await response.json()) as Task[\"batchEmbedContents\"][\"response\"]\n  return responseJson\n}\n\nasync function makeRequest(\n  url: URL,\n  body: string | undefined,\n  requestOptions?: RequestOptions,\n  requestMethod = \"POST\",\n  headers: Record<string, string> = {},\n): Promise<Response> {\n  let response: Response\n  try {\n    response = await fetch(url, {\n      ...buildFetchOptions(requestOptions),\n      method: requestMethod,\n      headers: {\n        \"Content-Type\": \"application/json\",\n        ...headers,\n      },\n      body,\n    })\n    if (!response.ok) {\n      let message: string | undefined = \"\"\n      try {\n        const errResp = (await response.json()) as components[\"schemas\"][\"Operation\"]\n        message = errResp.error?.message\n        if (errResp?.error?.details) {\n          message += ` ${JSON.stringify(errResp.error.details)}`\n        }\n      } catch (_e) {\n        // ignored\n      }\n      throw new Error(`[${response.status} ${response.statusText}] ${message}`)\n    }\n  } catch (e) {\n    console.log(e)\n    const err = new GoogleGenerativeAIError(`Error fetching from google -> ${e.message}`)\n    err.stack = e.stack\n    throw err\n  }\n  return response\n}\n\nconst BASE_URL = \"https://generativelanguage.googleapis.com\"\n\nfunction toURL({\n  model,\n  task,\n  stream,\n  apiParam,\n}: {\n  model: GeminiModel\n  task: keyof Task\n  stream: boolean\n  apiParam: ApiParam\n}) {\n  const api_version = model.apiVersion()\n  const url = new URL(`${BASE_URL}/${api_version}/models/${model}:${task}`)\n  url.searchParams.append(\"key\", apiParam.apikey)\n  if (stream) {\n    url.searchParams.append(\"alt\", \"sse\")\n  }\n  return url\n}\n\n/**\n * Generates the request options to be passed to the fetch API.\n * @param requestOptions - The user-defined request options.\n * @returns The generated request options.\n */\nfunction buildFetchOptions(requestOptions?: RequestOptions): RequestInit {\n  const fetchOptions = {} as RequestInit\n  if (requestOptions?.timeout) {\n    const abortController = new AbortController()\n    const signal = abortController.signal\n    setTimeout(() => abortController.abort(), requestOptions.timeout)\n    fetchOptions.signal = signal\n  }\n  return fetchOptions\n}\n", "import { GoogleGenerativeAIResponseError } from \"./errors.ts\"\nimport type { Candidate, FinishReason, FunctionCall, GenerateContentResponse } from \"./types.ts\"\n\n/**\n * Adds convenience helper methods to a response object, including stream\n * chunks (as long as each chunk is a complete GenerateContentResponse JSON).\n */\nexport function resultHelper(response: GenerateContentResponse): string | FunctionCall {\n  if (response.candidates && response.candidates.length > 0) {\n    if (response.candidates.length > 1) {\n      console.warn(\n        `This response had ${response.candidates.length} candidates. Returning text from the first candidate only. Access response.candidates directly to use the other candidates.`,\n      )\n    }\n    if (hadBadFinishReason(response.candidates[0])) {\n      throw new GoogleGenerativeAIResponseError<GenerateContentResponse>(\n        `${formatBlockErrorMessage(response)}`,\n        response,\n      )\n    }\n    return getText(response)\n  }\n  if (response.promptFeedback) {\n    throw new GoogleGenerativeAIResponseError<GenerateContentResponse>(\n      `Text not available. ${formatBlockErrorMessage(response)}`,\n      response,\n    )\n  }\n  return \"\"\n}\n\n/**\n * Returns text of first candidate.\n */\nexport function getText(response: GenerateContentResponse): string | FunctionCall {\n  if (response.candidates?.[0].content?.parts?.[0]?.text) {\n    return response.candidates[0].content.parts[0].text\n  }\n  if (response.candidates?.[0].content?.parts?.[0]?.functionCall) {\n    return response.candidates[0].content.parts[0].functionCall\n  }\n  return \"\"\n}\n\nconst badFinishReasons: FinishReason[] = [\"RECITATION\", \"SAFETY\"]\n\nfunction hadBadFinishReason(candidate: Candidate): boolean {\n  return !!candidate.finishReason && badFinishReasons.includes(candidate.finishReason)\n}\n\nfunction formatBlockErrorMessage(response: GenerateContentResponse): string {\n  let message = \"\"\n  if ((!response.candidates || response.candidates.length === 0) && response.promptFeedback) {\n    message += \"Response was blocked\"\n    if (response.promptFeedback?.blockReason) {\n      message += ` due to ${response.promptFeedback.blockReason}`\n    }\n    if (response.promptFeedback?.blockReasonMessage) {\n      message += `: ${response.promptFeedback.blockReasonMessage}`\n    }\n  } else if (response.candidates?.[0]) {\n    const firstCandidate = response.candidates[0]\n    if (hadBadFinishReason(firstCandidate)) {\n      message += `Candidate was blocked due to ${firstCandidate.finishReason}`\n      if (firstCandidate.finishMessage) {\n        message += `: ${firstCandidate.finishMessage}`\n      }\n    }\n  }\n  return message\n}\n", "import { streamGenerateContent } from \"../../../gemini-api-client/gemini-api-client.ts\"\nimport { resultHelper } from \"../../../gemini-api-client/response-helper.ts\"\nimport type { FunctionCall } from \"../../../gemini-api-client/types.ts\"\nimport type { Logger } from \"../../../log.ts\"\nimport type { OpenAI } from \"../../../types.ts\"\nimport { type ApiParam, genModel } from \"../../../utils.ts\"\n\nexport async function nonStreamingChatProxyHandler(\n  req: OpenAI.Chat.ChatCompletionCreateParams,\n  apiParam: ApiParam,\n  log?: Logger,\n): Promise<Response> {\n  const [model, geminiReq] = genModel(req)\n  let geminiResp: string | FunctionCall = \"\"\n\n  try {\n    for await (const it of streamGenerateContent(apiParam, model, geminiReq)) {\n      const data = resultHelper(it)\n      if (typeof data === \"string\") {\n        geminiResp += data\n      } else {\n        geminiResp = data\n        break\n      }\n    }\n  } catch (err) {\n    // Âá∫Áé∞ÂºÇÂ∏∏Êó∂ÊâìÂç∞ËØ∑Ê±ÇÂèÇÊï∞ÂíåÂìçÂ∫îÔºå‰ª•‰æøË∞ÉËØï\n    log?.error(req)\n    log?.error(err?.message ?? err.toString())\n    geminiResp = err?.message ?? err.toString()\n  }\n\n  log?.debug(req)\n  log?.debug(geminiResp)\n\n  function genOpenAiResp(content: string | FunctionCall): OpenAI.Chat.ChatCompletion {\n    if (typeof content === \"string\") {\n      return {\n        id: \"chatcmpl-abc123\",\n        object: \"chat.completion\",\n        created: Math.floor(Date.now() / 1000),\n        model: model.model,\n        choices: [\n          {\n            message: { role: \"assistant\", content: content, refusal: null },\n            finish_reason: \"stop\",\n            index: 0,\n            logprobs: null,\n          },\n        ],\n      }\n    }\n\n    return {\n      id: \"chatcmpl-abc123\",\n      object: \"chat.completion\",\n      created: Math.floor(Date.now() / 1000),\n      model: model.model,\n      choices: [\n        {\n          message: {\n            role: \"assistant\",\n            refusal: null,\n            content: null,\n            function_call: {\n              name: content.name ?? \"\",\n              arguments: JSON.stringify(content.args),\n            },\n          },\n          finish_reason: \"function_call\",\n          index: 0,\n          logprobs: null,\n        },\n      ],\n    }\n  }\n\n  return Response.json(genOpenAiResp(geminiResp))\n}\n", "import { streamGenerateContent } from \"../../../gemini-api-client/gemini-api-client.ts\"\nimport { resultHelper } from \"../../../gemini-api-client/response-helper.ts\"\nimport type { FunctionCall } from \"../../../gemini-api-client/types.ts\"\nimport type { Logger } from \"../../../log.ts\"\nimport type { OpenAI } from \"../../../types.ts\"\nimport { type ApiParam, genModel } from \"../../../utils.ts\"\n\nexport function streamingChatProxyHandler(\n  req: OpenAI.Chat.ChatCompletionCreateParams,\n  apiParam: ApiParam,\n  log?: Logger,\n): Response {\n  const [model, geminiReq] = genModel(req)\n  log?.debug(\"streamGenerateContent request\", req)\n  return sseResponse(\n    (async function* () {\n      try {\n        for await (const it of streamGenerateContent(apiParam, model, geminiReq)) {\n          log?.debug(\"streamGenerateContent resp\", it)\n          const data = resultHelper(it)\n          yield genStreamResp({\n            model: model.model,\n            content: data,\n            stop: false,\n          })\n        }\n      } catch (error) {\n        yield genStreamResp({\n          model: model.model,\n          content: error?.message ?? error.toString(),\n          stop: true,\n        })\n      }\n      yield genStreamResp({ model: model.model, content: \"\", stop: true })\n      yield \"[DONE]\"\n      return undefined\n    })(),\n  )\n}\n\nfunction genStreamResp({\n  model,\n  content,\n  stop,\n}: {\n  model: string\n  content: string | FunctionCall\n  stop: boolean\n}): OpenAI.Chat.ChatCompletionChunk {\n  if (typeof content === \"string\") {\n    return {\n      id: \"chatcmpl-abc123\",\n      object: \"chat.completion.chunk\",\n      created: Math.floor(Date.now() / 1000),\n      model: model,\n      choices: [\n        {\n          delta: { role: \"assistant\", content },\n          finish_reason: stop ? \"stop\" : null,\n          index: 0,\n        },\n      ],\n    } satisfies OpenAI.Chat.ChatCompletionChunk\n  }\n\n  return {\n    id: \"chatcmpl-abc123\",\n    object: \"chat.completion.chunk\",\n    created: Math.floor(Date.now() / 1000),\n    model: model,\n    choices: [\n      {\n        delta: { role: \"assistant\", function_call: content },\n        finish_reason: stop ? \"function_call\" : null,\n        index: 0,\n      },\n    ],\n  } satisfies OpenAI.Chat.ChatCompletionChunk\n}\n\nconst encoder = new TextEncoder()\n\nfunction sseResponse(dataStream: AsyncGenerator<string | OpenAI.Chat.ChatCompletionChunk, undefined>): Response {\n  const s = new ReadableStream<Uint8Array>({\n    async pull(controller) {\n      const { value, done } = await dataStream.next()\n      if (done) {\n        controller.close()\n      } else {\n        const data = typeof value === \"string\" ? value : JSON.stringify(value)\n        controller.enqueue(encoder.encode(toSseMsg({ data })))\n      }\n    },\n  })\n\n  const response = new Response(s, {\n    status: 200,\n    headers: new Headers({\n      \"Content-Type\": \"text/event-stream\",\n    }),\n  })\n\n  return response\n}\n\nexport function toSseMsg({ event, data, id }: SseEvent) {\n  let result = `data: ${data}\\n`\n  if (event) {\n    result += `event: ${event ?? \"\"}\\n`\n  }\n  if (id) {\n    result += `id: ${id ?? \"\"}\\n`\n  }\n  return `${result}\\n`\n}\n\nexport interface SseEvent {\n  event?: string\n  id?: string\n  data: string\n}\n", "import type { OpenAI } from \"../../../types.ts\"\nimport { getToken } from \"../../../utils.ts\"\nimport { nonStreamingChatProxyHandler } from \"./NonStreamingChatProxyHandler.ts\"\nimport { streamingChatProxyHandler } from \"./StreamingChatProxyHandler.ts\"\n\nexport async function chatProxyHandler(rawReq: Request): Promise<Response> {\n  const req = (await rawReq.json()) as OpenAI.Chat.ChatCompletionCreateParams\n  const headers = rawReq.headers\n  const apiParam = getToken(headers)\n  if (apiParam == null) {\n    return new Response(\"Unauthorized\", { status: 401 })\n  }\n\n  if (req.stream !== true) {\n    return await nonStreamingChatProxyHandler(req, apiParam, rawReq.logger)\n  }\n  return streamingChatProxyHandler(req, apiParam, rawReq.logger)\n}\n", "import type { BatchEmbedContentsRequest } from \"../gemini-api-client/gemini-api-client.ts\"\nimport { batchEmbedContents } from \"../gemini-api-client/gemini-api-client.ts\"\nimport type { OpenAI } from \"../types.ts\"\nimport { GeminiModel, getToken } from \"../utils.ts\"\n\n//https://ai.google.dev/gemini-api/docs/embeddings#javascript_1\n//https://ai.google.dev/gemini-api/docs/embeddings#control-embedding-size\nconst GEMINI_EMBEDDING_MODEL = \"text-embedding-004\"\n//\"embedding-001\"\n//\"gemini-embedding-001\"\nconst GEMINI_EMBEDDING_MODEL_OUTPUT_DIM = 768\n// Google API's recommended batch size limit is 100\nconst BATCH_SIZE = 100\n//https://github.com/lobehub/lobe-chat/issues/8482\n//change database can solve this problem\n//alter table embeddings alter column embeddings type vector(768);\nexport async function embeddingProxyHandler(rawReq: Request): Promise<Response> {\n  const req = (await rawReq.json()) as OpenAI.Embeddings.EmbeddingCreateParams\n  const log = rawReq.logger\n  const headers = rawReq.headers\n  const apiParam = getToken(headers)\n  if (apiParam == null) {\n    return new Response(\"Unauthorized\", { status: 401 })\n  }\n\n  // batch\n  const inputs = [req.input].flat().map((it) => it.toString())\n  const allEmbeddings: number[][] = []\n\n  const modelIdentifier = `models/${GEMINI_EMBEDDING_MODEL}`\n  try {\n    // Loop through the inputs in chunks of BATCH_SIZE\n    for (let i = 0; i < inputs.length; i += BATCH_SIZE) {\n      const batchInputs = inputs.slice(i, i + BATCH_SIZE)\n\n      log?.warn(`Processing batch of ${batchInputs.length} inputs... (starting at index ${i})`)\n\n      // Construct the request payload for batchEmbedContents\n      const batchRequest: BatchEmbedContentsRequest = {\n        // The `requests` field is an array of individual embedding requests\n        requests: batchInputs.map((text) => ({\n          model: modelIdentifier,\n          content: {\n            parts: [{ text }],\n          },\n        })),\n      }\n\n      // Call the new, efficient batching function\n      const response = await batchEmbedContents(apiParam, new GeminiModel(GEMINI_EMBEDDING_MODEL), batchRequest)\n\n      // The response contains a list of embeddings, in the same order as the requests\n      if (response.embeddings) {\n        const embeddingsForBatch = response.embeddings.map((emb) => emb.values)\n        allEmbeddings.push(...embeddingsForBatch)\n      }\n    }\n\n    // Now, format the collected embeddings into the OpenAI-compatible response structure\n    const responseData = allEmbeddings.map((embedding, index) => ({\n      object: \"embedding\",\n      index: index,\n      embedding: embedding,\n    }))\n\n    const finalResponse: OpenAI.Embeddings.CreateEmbeddingResponse = {\n      object: \"list\",\n      data: responseData,\n      model: req.model,\n      usage: {\n        prompt_tokens: 0, // Note: You would need to implement token counting separately\n        total_tokens: 0,\n      },\n    }\n\n    return Response.json(finalResponse)\n  } catch (err) {\n    log?.error(\"Request failed:\", req)\n    log?.error(\"Error details:\", err?.message ?? err.toString())\n    return new Response(JSON.stringify({ error: { message: err.message } }), {\n      status: 500,\n      headers: { \"Content-Type\": \"application/json\" },\n    })\n  }\n\n  // batch\n  /*\n  \n  \n    const embedContentRequest: EmbedContentRequest = {\n      model:  GEMINI_EMBEDDING_MODEL,\n      content: {\n        parts: [req.input].flat().map((it) => ({ text: it.toString() })),\n      },\n      outputDimensionality: GEMINI_EMBEDDING_MODEL_OUTPUT_DIM \n    }\n  \n    log?.warn(\"request\", embedContentRequest)\n  \n    let geminiResp: number[] | undefined = []\n  \n    try {\n      const it = await embedContent(apiParam, new GeminiModel(GEMINI_EMBEDDING_MODEL), embedContentRequest)\n      const data = it?.embedding?.values\n      geminiResp = data\n    } catch (err) {\n      // Âá∫Áé∞ÂºÇÂ∏∏Êó∂ÊâìÂç∞ËØ∑Ê±ÇÂèÇÊï∞ÂíåÂìçÂ∫îÔºå‰ª•‰æøË∞ÉËØï\n      log?.error(req)\n      log?.error(err?.message ?? err.toString())\n      geminiResp = err?.message ?? err.toString()\n    }\n  \n    log?.debug(req)\n    log?.debug(geminiResp)\n  \n    const resp: OpenAI.Embeddings.CreateEmbeddingResponse = {\n      object: \"list\",\n      data: [\n        {\n          object: \"embedding\",\n          index: 0,\n          embedding: geminiResp ?? [],\n        },\n      ],\n      model: req.model,\n      usage: {\n        prompt_tokens: 5,\n        total_tokens: 5,\n      },\n    }\n  \n    return Response.json(resp)\n  */\n}\n", "import { listModels } from \"../gemini-api-client/gemini-api-client.ts\"\nimport type { OpenAI } from \"../types.ts\"\nimport { getToken, ModelMapping } from \"../utils.ts\"\nexport const modelData: OpenAI.Models.Model[] = Object.keys(ModelMapping).map((model) => ({\n  created: 1677610602,\n  object: \"model\",\n  owned_by: \"openai\",\n  id: model,\n}))\n\nexport const models = async (req: Request) => {\n  const apiParam = getToken(req.headers)\n  const gemini_models = await listModels(apiParam)\n  const model_list = gemini_models[\"data\"] as Array<any>\n  const finala_model_list = model_list.concat(modelData)\n\n  return {\n    object: \"list\",\n    data: distinctArrayByKey(finala_model_list, (item) => item.id),\n  }\n}\n\nfunction distinctArrayByKey<T>(arr: T[], keySelector: (item: T) => any): T[] {\n  const seen = new Set()\n  return arr.filter((item) => {\n    const value = keySelector(item)\n    if (seen.has(value)) {\n      return false\n    }\n    seen.add(value)\n    return true\n  })\n}\n\nexport const modelDetail = (model: string) => {\n  return modelData.find((it) => it.id === model)\n}\n", "import type { IRequest } from \"itty-router/\"\nimport { cors } from \"itty-router/cors\"\nimport { Router } from \"itty-router/Router\"\nimport { geminiProxy } from \"./gemini-proxy.ts\"\nimport { hello } from \"./hello.ts\"\nimport { type Any, Logger } from \"./log.ts\"\nimport { ttsProxyHandler } from \"./openai/audio/speech/TTSProxyHandler.ts\"\nimport { chatProxyHandler } from \"./openai/chat/completions/ChatProxyHandler.ts\"\nimport { embeddingProxyHandler } from \"./openai/embeddingProxyHandler.ts\"\nimport { modelDetail, models } from \"./openai/models.ts\"\n\nconst { preflight, corsify } = cors({ allowHeaders: \"*\" })\n\nconst app = Router<IRequest, Any[], Response>({\n  before: [\n    preflight,\n    (req) => {\n      req.logger = new Logger(crypto.randomUUID().toString())\n      req.logger.warn(`--> ${req.method} ${req.url}`)\n    },\n  ],\n  finally: [\n    corsify,\n    (_, req) => {\n      req.logger?.warn(`<-- ${req.method} ${req.url}`)\n      // return resp\n    },\n  ],\n})\n\napp.get(\"/\", hello)\napp.post(\"/v1/chat/completions\", chatProxyHandler)\napp.post(\"/v1/audio/speech\", ttsProxyHandler)\napp.post(\"/v1/embeddings\", embeddingProxyHandler)\napp.get(\"/v1/models\", async (req) => Response.json(await models(req)))\napp.get(\"/v1/models/:model\", (c) => Response.json(modelDetail(c.params.model)))\napp.post(\"/:model_version/models/:model_and_action\", geminiProxy)\napp.all(\"*\", () => new Response(\"Page Not Found\", { status: 404 }))\n\nexport { app }\n", "import { app } from \"./src/app.ts\"\n\nconsole.log(\"Listening on http://localhost:8000/\")\n\n// @ts-expect-error supress warning\nBun.serve({\n  port: 8000,\n  fetch: app.fetch,\n})\n"],
  "mappings": ";IAoBaA,IAAQC,CAAuBC,KAAA,CAAA,MAE1C;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ICdWC,IAITC,CAAAA,EAAAA,MAAOC,KAAIC,IAAAA,QAAgBC,KAA4C,CAAA,GAAA,GAEvEC,EAAAA,IAAAA,CAAAA,OAAW;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ACdf,eAAsB,YAAY,QAAe;AAC/C,QAAM,MAAM,IAAI,IAAI,OAAO,GAAG;AAC9B,MAAI,OAAO;AACX,MAAI,OAAO;AACX,MAAI,WAAW;AACf,QAAM,MAAM,IAAI,QAAQ,KAAK,MAAA;AAC7B,QAAM,OAAO,MAAM,MAAM,GAAA;AACzB,SAAO,IAAI,SAAS,KAAK,MAAM,IAAA;AACjC;;;ACCO,SAAS,SAAS,SAAmC;AAC1D,aAAW,CAAC,GAAG,CAAA,KAAM,SAAS;AAC5B,QAAI,EAAE,YAAW,MAAO,gBAAiB;AAEzC,UAAM,YAAY,EAAE,UAAU,EAAE,QAAQ,GAAA,IAAO,CAAA;AAE/C,QAAI,CAAC,UAAU,SAAS,GAAA,GAAM;AAC5B,aAAO;QACL,QAAQ;QACR,SAAS;MACX;IACF;AAGA,UAAM,SAAS,UAAU,UAAU,GAAG,UAAU,QAAQ,GAAA,CAAA;AACxD,UAAM,SAAS,IAAI,gBAAgB,UAAU,UAAU,UAAU,QAAQ,GAAA,IAAO,CAAA,CAAA;AAChF,WAAO;MACL;MACA,SAAS,OAAO,IAAI,SAAA;IACtB;EACF;AACA,SAAO;AACT;AAEA,SAAS,YAAY,QAAc;AACjC,MAAI,CAAC,OAAO,WAAW,OAAA,GAAU;AAC/B,WAAO;MAAE,MAAM;IAAG;EACpB;AAEA,QAAM,CAAC,GAAG,MAAM,GAAG,IAAA,IAAQ,OAAO,MAAM,GAAA;AACxC,QAAM,WAAW,EAAE,MAAM,gBAAA,GAAmB,QAAQ,QAAQ;AAC5D,SAAO;IACL,YAAY;MACV;MACA;IACF;EACF;AACF;AAEO,SAAS,6BAA6B,UAAkD;AAC7F,QAAM,SAAoB,SAAS,QAAQ,CAAC,EAAE,MAAM,QAAO,MAAE;AAC3D,QAAI,SAAS,UAAU;AACrB,aAAO;QACL;UACE,MAAM;UACN,OAAO,OAAO,YAAY,WAAW,UAAU;YAAC;cAAE,MAAM;YAAQ;;QAClE;;IAEJ;AACA,UAAM,QACJ,WAAW,QAAQ,OAAO,YAAY,WAClC;MAAC;QAAE,MAAM,SAAS,SAAA,KAAc;MAAG;QACnC,QAAQ,IAAI,CAAC,SAAA;AACX,UAAI,KAAK,SAAS,OAAQ,QAAO;QAAE,MAAM,KAAK;MAAK;AACnD,UAAI,KAAK,SAAS,YAAa,QAAO,YAAY,KAAK,UAAU,GAAG;AACpE,aAAO;QAAE,MAAM;MAAK;IACtB,CAAA;AACN,WAAO;MAAC;QAAE,MAAM,WAAW,OAAO,SAAS;QAAS;MAAa;;EACnE,CAAA;AAEA,SAAO;AACT;AAEO,SAAS,SAAS,KAA2C;AAClE,QAAM,QAAqB,YAAY,aAAa,IAAI,KAAK;AAE7D,MAAI,YACF,IAAI,OAAO,OAAO,CAAC,OAAO,GAAG,SAAS,UAAA,GAAa,IAAI,CAAC,OAAO,GAAG,QAAQ,KAAK,CAAA;AAEjF,cAAY,UAAU,QAAQ,IAAI,aAAa,CAAA,GAAI,IAAI,CAAC,QAAQ;IAAE,QAAQ;IAAM,GAAG;EAAG,EAAC,CAAA;AAEvF,QAAM,CAAC,kBAAkB,cAAA,KAAmB,MAAA;AAC1C,YAAQ,IAAI,iBAAiB,MAAA;MAC3B,KAAK;AACH,eAAO;UAAC;UAAoB;;MAC9B,KAAK;AACH,eAAO;UAAC;UAAoB,IAAI,gBAAgB,YAAY;;MAC9D,KAAK;AACH,eAAO;UAAC;UAAc;;MACxB;AACE,eAAO;UAAC;UAAW;;IACvB;EACF,GAAC;AAED,QAAM,yBAAiD;IACrD,UAAU,6BAA6B,IAAI,QAAQ;IACnD,kBAAkB;MAChB,iBAAiB,IAAI,yBAAyB;MAC9C,aAAa,IAAI,eAAe;MAChC,MAAM,IAAI,SAAS;MACnB;MACA;MACA,gBAAgB,CAAC,MAAM,gBAAe,IAClC,SACA;QACE,iBAAiB;MACnB;IACN;IACA,OACE,UAAU,WAAW,IACjB,SACA;MACE;QACE,sBAAsB;MACxB;;IAER,gBACE;MACE;MACA;MACA;MACA;MAEF,IAAI,CAAC,cAAc;MACnB;MACA,WAAW;IACb,EAAC;EACH;AACA,SAAO;IAAC;IAAO;;AACjB;AAaA,IAAM,gCAAgC;AAG/B,IAAM,cAAN,MAAM,aAAA;EACX,OAAO,aAAa,OAAwC;AAC1D,UAAM,YACJ,aAAa,SAAS,EAAA,KAAO,aAAY,aAAa,SAAS,EAAA;AACjE,WAAO,IAAI,aAAY,SAAA;EACzB;EACgB;EAChB,YAAY,OAAwB;AAClC,SAAK,QAAQ;EACf;EAEA,kBAA2B;AACzB,WAAO,KAAK,MAAM,SAAS,UAAA;EAC7B;EAEA,aAA0B;AACxB,QAAI,KAAK,gBAAe,GAAI;AAC1B,aAAO;IACT;AACA,WAAO;EACT;EAEA,WAAmB;AACjB,WAAO,KAAK;EACd;EAEA,OAAe,aAAa,GAA4B;AACtD,QAAI,EAAE,WAAW,QAAA,KAAa,EAAE,WAAW,OAAA,GAAU;AACnD,aAAO;IACT;AACA,WAAO;EACT;AACF;AAYO,IAAM,eAA2D;;EAEtE,iBAAiB;EACjB,UAAU;EACV,eAAe;EACf,SAAS;EACT,wBAAwB;EACxB,eAAe;EACf,uBAAuB;EACvB,gBAAgB;EAChB,gBAAgB;EAChB,WAAW;EACX,cAAc;EACd,cAAc;EACd,SAAS;;EAGT,0BAA0B;EAC1B,0BAA0B;EAC1B,0BAA0B;;EAG1B,SAAS;AACX;AAEO,SAAS,gBAAA;AACd,QAAM,SAAS;AACf,MAAI,QAAQ,SAAS,QAAW;AAC9B,WAAO;EACT;AACA,MAAI,QAAQ,QAAQ,QAAW;AAC7B,WAAO;EACT;AACA,MAAI,OAAO,QAAQ,kBAAkB,YAAY;AAC/C,WAAO;EACT;AACA,MAAI,OAAO,QAAQ,gBAAgB,UAAU;AAC3C,WAAO;EACT;AACA,MAAI,QAAQ,WAAW,QAAW;AAChC,WAAO;EACT;AACA,MAAI,QAAQ,SAAS,SAAS,SAAS,QAAQ;AAC7C,WAAO;EACT;AACA,SAAO;AACT;;;ACxOO,SAAS,MAAM,KAAY;AAChC,QAAM,SAAS,IAAI,IAAI,IAAI,GAAG,EAAE;AAChC,SAAO,IAAI,SAAS;qCACe,cAAA,CAAA;;;;WAI1B,MAAA;;;;;;;;KAQN;AACL;;;ACTA,IAAM,QAAQ;EAAC;EAAS;EAAQ;EAAQ;;AAOjC,IAAM,SAAN,MAAM;EACH;EAER;EACA;EACA;EACA;EAEA,YAAY,QAAiB,UAAmB;AAC9C,UAAM,QAAQ,MAAM,KAAK,CAAC,OAAO,OAAO,QAAA,KAAa;AACrD,SAAK,SAAS;MACZ,QAAQ,UAAU;MAClB;IACF;AAEA,eAAW,KAAK,OAAO;AACrB,WAAK,CAAA,IAAK,IAAI,SAAgB,KAAK,OAAO,GAAA,GAAM,IAAA;IAClD;EACF;EAEA,OAAO,UAA2B,MAAW;AAC3C,UAAM,EAAE,OAAO,aAAa,OAAM,IAAK,KAAK;AAC5C,QAAI,MAAM,QAAQ,KAAA,IAAS,MAAM,QAAQ,WAAA,GAAc;AACrD;IACF;AAEA,YAAQ,KAAA,EAAO,IAAG,oBAAI,KAAA,GAAO,YAAW,CAAA,IAAM,MAAM,YAAW,CAAA,GAAK,SAAS,IAAI,MAAA,KAAW,EAAA,IAAI,GAAK,IAAA;EACvG;AACF;;;ACpCA,eAAe,WAAW,KAAiB,MAAY;AACrD,QAAM,YAAY,MAAM,OAAO,OAAO,UAAU,OAAO,KAAK;IAAE,MAAM;IAAQ,MAAM;MAAE,MAAM;IAAU;EAAE,GAAG,OAAO;IAC9G;GACD;AACD,QAAM,YAAY,MAAM,OAAO,OAAO,KAAK,QAAQ,WAAW,IAAI,YAAA,EAAc,OAAO,IAAA,CAAA;AACvF,SAAO,IAAI,WAAW,SAAA;AACxB;AAEA,eAAe,cAAc,QAAc;AACzC,QAAM,eAAe,KAAK,MAAA;AAC1B,QAAM,QAAQ,IAAI,WAAW,aAAa,MAAM;AAChD,WAAS,IAAI,GAAG,IAAI,aAAa,QAAQ,KAAK;AAC5C,UAAM,CAAA,IAAK,aAAa,WAAW,CAAA;EACrC;AACA,SAAO;AACT;AAEA,eAAe,cAAc,OAAiB;AAC5C,SAAO,KAAK,OAAO,aAAa,MAAM,MAAM,MAAM,KAAK,KAAA,CAAA,CAAA;AACzD;AAEA,SAAS,OAAA;AACP,SAAO,OAAO,WAAU,EAAG,QAAQ,MAAM,EAAA;AAC3C;AAEA,eAAsB,KAAK,QAAc;AACvC,QAAM,MAAM,OAAO,MAAM,KAAA,EAAO,CAAA;AAChC,QAAM,aAAa,mBAAmB,GAAA;AACtC,QAAM,UAAU,KAAA;AAChB,QAAM,gBAAgB,WAAA;AACtB,QAAM,cAAc,yBAAyB,UAAA,GAAa,aAAA,GAAgB,OAAA,GAAU,YAAW;AAC/F,QAAM,SAAS,MAAM,cACnB,0FAAA;AAEF,QAAM,WAAW,MAAM,WAAW,QAAQ,WAAA;AAC1C,QAAM,aAAa,MAAM,cAAc,QAAA;AACvC,SAAO,2BAA2B,UAAA,KAAe,aAAA,KAAkB,OAAA;AACrE;AAEA,SAAS,aAAA;AACP,QAAM,iBAAgB,oBAAI,KAAA,GAAO,YAAW,EAAG,QAAQ,OAAO,EAAA,EAAI,KAAI,IAAK;AAC3E,SAAO,cAAc,YAAW;AAClC;AAEO,SAAS,cAAc,KAAa,WAAmB,UAAQ;AACpE,QAAM,YAAY,IAAI,MAAM,QAAA;AAC5B,QAAM,SAAwB,CAAA;AAC9B,MAAI,eAA8B,CAAA;AAClC,MAAI,gBAAgB;AAEpB,aAAW,YAAY,WAAW;AAChC,QAAI,gBAAgB,SAAS,UAAU,aAAa,SAAS,IAAI,IAAI,MAAM,WAAW;AACpF,mBAAa,KAAK,QAAA;AAClB,uBAAiB,SAAS,UAAU,aAAa,SAAS,IAAI,IAAI;IACpE,OAAO;AACL,UAAI,aAAa,SAAS,GAAG;AAC3B,eAAO,KAAK,aAAa,KAAK,GAAA,CAAA;MAChC;AACA,qBAAe;QAAC;;AAChB,sBAAgB,SAAS;IAC3B;EACF;AAEA,MAAI,aAAa,SAAS,GAAG;AAC3B,WAAO,KAAK,aAAa,KAAK,GAAA,CAAA;EAChC;AAEA,SAAO;AACT;AAEO,SAAS,kBAAA;AACd,SAAO;IACL,+BAA+B;IAC/B,gCAAgC;IAChC,gCAAgC;IAChC,0BAA0B;EAC5B;AACF;;;ACnFA,IAAM,uBAAuB;AAC7B,IAAM,qBACJ;AAEF,IAAI,aAAuB,CAAA;AAC3B,IAAM,8BAA8B,IAAI;AACxC,IAAI,YAIA;EACF,UAAU;EACV,OAAO;EACP,WAAW;AACb;AAEA,IAAI,wBAAwB;AAC5B,IAAI,gBAA0C;AAU9C,eAAe,WAAA;AACb,QAAM,eAAe,MAAM,YAAA;AAC3B,QAAM,WAAW,MAAM,MAAM,oBAAoB;IAC/C,QAAQ;IACR,SAAS;MACP,eAAe,cAAc,KAAK;MAClC,gBAAgB;MAChB,cACE;MACF,4BAA4B;IAC9B;EACF,CAAA;AAEA,QAAM,OAAO,MAAM,SAAS,KAAI;AAChC,eAAa,KAAK,IAAI,CAAC,MAAW,EAAE,WAAA,CAAY;AAChD,QAAM,gBAAgB,WAAW,OAAO,CAAC,MAAM,EAAE,WAAW,IAAA,CAAA;AAC5D,QAAM,gBAAgB,WAAW,OAAO,CAAC,MAAM,EAAE,WAAW,IAAA,CAAA;AAC5D,QAAM,gBAAgB,WAAW,OAAO,CAAC,MAAM,EAAE,WAAW,IAAA,CAAA;AAC5D,eAAa,cAAc,OAAO,cAAc,OAAO,aAAA,CAAA;AACvD,UAAQ,MAAM,UAAA;AAChB;AAEA,eAAe,cAAA;AACb,MAAI,eAAe;AACjB,WAAO;EACT;AACA,kBAAgB,MAAM,aAAA;AAEtB,SAAO;AACT;AAEA,eAAe,eAAA;AACb,QAAM,MAAM,KAAK,IAAG,IAAK;AAEzB,MAAI,UAAU,SAAS,UAAU,aAAa,MAAM,UAAU,YAAY,6BAA6B;AACrG,WAAO,UAAU;EACnB;AAGA,QAAM,cAAc;AACpB,QAAM,WAAW,OAAO,WAAU,EAAG,QAAQ,MAAM,EAAA;AAEnD,MAAI;AACF,UAAM,WAAW,MAAM,MAAM,aAAa;MACxC,QAAQ;MACR,SAAS;QACP,mBAAmB;QACnB,mBAAmB;QACnB,YAAY;QACZ,0BAA0B;QAC1B,mBAAmB;QACnB,kBAAkB,MAAM,KAAK,WAAA;QAC7B,cACE;QACF,gBAAgB;QAChB,kBAAkB;QAClB,mBAAmB;MACrB;IACF,CAAA;AAEA,QAAI,CAAC,SAAS,IAAI;AAChB,YAAM,IAAI,MAAM,qCAAiB,SAAS,MAAM,EAAE;IACpD;AAEA,UAAM,OAAQ,MAAM,SAAS,KAAI;AACjC,UAAM,MAAM,KAAK,EAAE,MAAM,GAAA,EAAK,CAAA;AAC9B,UAAM,aAAa,KAAK,MAAM,KAAK,GAAA,CAAA;AAEnC,gBAAY;MACV,UAAU;MACV,OAAO,KAAK;MACZ,WAAW,WAAW;IACxB;AAEA,WAAO;EACT,SAAS,OAAO;AACd,YAAQ,MAAM,qCAAiB,KAAA;AAE/B,QAAI,UAAU,OAAO;AACnB,cAAQ,IAAI,iDAAA;AACZ,aAAO,UAAU;IACnB;AACA,UAAM;EACR;AACF;AAEA,SAAS,QACP,MACA,WACA,MACA,OACA,QACA,OACA,QAAQ,GAAC;AAET,MAAI,YAAY;AAChB,MAAI,QAAQ,GAAG;AACb,gBAAY,gBAAgB,KAAA;EAC9B;AACA,SAAO;+BACsB,SAAA;+CACgB,KAAA;yCACN,IAAA,YAAgB,KAAA,aAAkB,MAAA,KAAW,IAAA;;sBAEhE,SAAA;;;AAGtB;AAYA,eAAe,oBAAoB,UAA0B;AAC3D,QAAM,eAAe,MAAM,YAAA;AAC3B,MAAI;AACF,UAAM,WAAW,MAAM,MAAM,uBAAuB;MAClD,QAAQ;MACR,SAAS;QACP,eAAe,cAAc,KAAK;QAClC,gBAAgB;QAChB,cACE;QACF,4BAA4B;MAC9B;MACA,MAAM,QACJ,SAAS,MACT,SAAS,WACT,SAAS,MACT,SAAS,OACT,SAAS,QACT,SAAS,OACT,SAAS,KAAK;IAElB,CAAA;AAEA,QAAI,CAAC,SAAS,IAAI;AAChB,YAAM,YAAY,MAAM,SAAS,KAAI;AACrC,cAAQ,MAAM,kBAAkB;QAC9B,QAAQ,SAAS;QACjB,YAAY,SAAS;QACrB,MAAM;QACN,SAAS,OAAO,YAAY,SAAS,QAAQ,QAAO,CAAA;MACtD,CAAA;AACA,cAAQ,MAAM,iBAAiB,QAAA;AAC/B,cAAQ,MAAM,6BAA6B,oBAAA;AAE3C,aAAO,IAAI,SACT,KAAK,UAAU;QACb,OAAO,kBAAkB,SAAS,MAAM,IAAI,SAAS,UAAU;QAC/D,SAAS;MACX,CAAA,GACA;QACE,QAAQ,SAAS;QACjB,SAAS;UAAE,gBAAgB;QAAmB;MAChD,CAAA;IAEJ;AAGA,WAAO,IAAI,SAAS,SAAS,MAAM;MACjC,QAAQ;MACR,SAAS;QACP,gBAAgB,SAAS,QAAQ,IAAI,cAAA,KAAmB;QACxD,kBAAkB,SAAS,QAAQ,IAAI,gBAAA,KAAqB;MAC9D;IACF,CAAA;EACF,SAAS,OAAO;AACd,YAAQ,MAAM,sBAAsB,KAAA;AACpC,WAAO,IAAI,SACT,KAAK,UAAU;MACb,OAAO;MACP,SAAS,iBAAiB,QAAQ,MAAM,UAAU;IACpD,CAAA,GACA;MACE,QAAQ;MACR,SAAS;QAAE,gBAAgB;MAAmB;IAChD,CAAA;EAEJ;AACF;AAEA,eAAsB,iBAAiB,KAAa;AAClD,QAAM,eAAe,MAAM,YAAA;AAC3B,0BAAwB,WAAW,cAAc,CAAA;AAEjD,MAAI,WAAW,WAAW,GAAG;AAC3B,UAAM,SAAA;EACR;AAEA,QAAM,eAAe;AACrB,QAAM,SAAS,cAAc,IAAI,MAAM,KAAI,GAAI,cAAc,IAAA;AAC7D,QAAM,cAA2B,CAAA;AACjC,SAAO,OAAO,SAAS,GAAG;AACxB,QAAI;AAIF,UAAI,QAAQ,IAAI;AAChB,UAAI,CAAC,WAAW,SAAS,KAAA,GAAQ;AAC/B,gBAAQ,WAAW,CAAA;MACrB;AAGA,YAAM,WAAW;QACf,MAAM,OAAO,MAAK;QAClB,WAAW;QACX,MAAM;QACN,OAAO;QACP,QAAQ;QACR,OAAO;QACP,OAAO;MACT;AACA,YAAM,cAAc,OAAO,MAAM,oBAAoB,QAAA,GAA+B,KAAI;AAExF,kBAAY,KAAK,WAAA;IACnB,SAAS,OAAO;AACd,cAAQ,MAAM,sBAAsB,KAAA;AACpC,aAAO,IAAI,SACT,KAAK,UAAU;QACb,OAAO;QACP,SAAS,iBAAiB,QAAQ,MAAM,UAAU;MACpD,CAAA,GACA;QACE,QAAQ;QACR,SAAS;UAAE,gBAAgB;QAAmB;MAChD,CAAA;IAEJ;EACF;AAEA,QAAM,oBAAoB,IAAI,KAAK,aAAa;IAAE,MAAM;EAAa,CAAA;AACrE,QAAM,WAAW,IAAI,SAAS,mBAAmB;IAC/C,SAAS;MACP,gBAAgB;MAChB,GAAG,gBAAA;IACL;EACF,CAAA;AAEA,SAAO;AACT;;;ACjRA,IAAM,uBAAuB;AAC7B,IAAMC,wBAAuB;AAI7B,IAAMC,cAAa;EAAC;EAAS;EAAO;EAAU;EAAS;EAAQ;EAAS;EAAQ;EAAQ;EAAQ;;AAEhG,IAAM,iBAAiB;;;;;;AAOvB,eAAsB,mBAAmB,UAAyB;AAChE,MAAI;AAMF,UAAM,WAAW,MAAM,MAAM,sBAAsB;MACjD,QAAQ;MACR,SAAS;;;;;;QAMP,cACE;;QAEF,gBAAgB;QAChB,QAAQ;QACR,mBAAmB;MACrB;;MAEA,MAAM,SAAS,SAAQ;IACzB,CAAA;AAEA,QAAI,CAAC,SAAS,IAAI;AAChB,YAAM,YAAY,MAAM,SAAS,KAAI;AACrC,cAAQ,MAAM,kBAAkB;QAC9B,QAAQ,SAAS;QACjB,YAAY,SAAS;QACrB,MAAM;QACN,SAAS,OAAO,YAAY,SAAS,QAAQ,QAAO,CAAA;MACtD,CAAA;AACA,cAAQ,MAAM,iBAAiB,QAAA;AAE/B,aAAO,IAAI,SACT,KAAK,UAAU;QACb,OAAO,kBAAkB,SAAS,MAAM,IAAI,SAAS,UAAU;QAC/D,SAAS;MACX,CAAA,GACA;QACE,QAAQ,SAAS;QACjB,SAAS;UAAE,gBAAgB;QAAmB;MAChD,CAAA;IAEJ;AAGA,WAAO,IAAI,SAAS,SAAS,MAAM;MACjC,QAAQ;MACR,SAAS;QACP,gBAAgB,SAAS,QAAQ,IAAI,cAAA,KAAmB;QACxD,kBAAkB,SAAS,QAAQ,IAAI,gBAAA,KAAqB;MAC9D;IACF,CAAA;EACF,SAAS,OAAO;AACd,YAAQ,MAAM,sBAAsB,KAAA;AACpC,WAAO,IAAI,SACT,KAAK,UAAU;MACb,OAAO;MACP,SAAS,iBAAiB,QAAQ,MAAM,UAAU;IACpD,CAAA,GACA;MACE,QAAQ;MACR,SAAS;QAAE,gBAAgB;MAAmB;IAChD,CAAA;EAEJ;AACF;AACA,eAAsB,gBAAgB,KAAa;AACjD,QAAM,eAAe;AACrB,QAAM,SAAS,cAAc,IAAI,MAAM,KAAI,GAAI,cAAc,IAAA;AAC7D,QAAM,cAA2B,CAAA;AACjC,SAAO,OAAO,SAAS,GAAG;AACxB,QAAI;AAEF,YAAM,aAAa,OAAO,WAAU,EAAG,SAAQ;AAG/C,UAAI,QAAQ,IAAI;AAChB,UAAI,CAACA,YAAW,SAAS,KAAA,GAAQ;AAC/B,gBAAQA,YAAW,CAAA;MACrB;AACA,YAAM,WAAW,IAAI,gBAAgB;QACnC,OAAO,OAAO,MAAK;QACnB;QACA;QACA,iBAAiB,IAAI,mBAAmBD;QACxC,QAAQ,IAAI,gBAAgB;MAC9B,CAAA;AACA,YAAM,cAAc,OAAO,MAAM,mBAAmB,QAAA,GAAW,KAAI;AAEnE,kBAAY,KAAK,WAAA;IACnB,SAAS,OAAO;AACd,cAAQ,MAAM,sBAAsB,KAAA;AACpC,aAAO,IAAI,SACT,KAAK,UAAU;QACb,OAAO;QACP,SAAS,iBAAiB,QAAQ,MAAM,UAAU;MACpD,CAAA,GACA;QACE,QAAQ;QACR,SAAS;UAAE,gBAAgB;QAAmB;MAChD,CAAA;IAEJ;EACF;AAEA,QAAM,oBAAoB,IAAI,KAAK,aAAa;IAAE,MAAM;EAAa,CAAA;AACrE,QAAM,WAAW,IAAI,SAAS,mBAAmB;IAC/C,SAAS;MACP,gBAAgB;MAChB,GAAG,gBAAA;IACL;EACF,CAAA;AAEA,SAAO;AACT;;;ACjIA,eAAsB,gBAAgB,QAAe;AACnD,QAAM,MAAO,MAAM,OAAO,KAAI;AAC9B,QAAM,UAAU,OAAO;AACvB,QAAM,WAAW,SAAS,OAAA;AAE1B,MAAI,YAAY,MAAM;AACpB,WAAO,IAAI,SAAS,gBAAgB;MAAE,QAAQ;IAAI,CAAA;EACpD;AAEA,MAAI,IAAI,UAAU,SAAS;AACzB,WAAO,gBAAgB,GAAA;EACzB;AACA,SAAO,iBAAiB,GAAA;AAC1B;;;ACPO,IAAM,aAAN,cAAyB,MAAM;EAqBpC,YACE,SACA,SAAA;AAEA,UAAM,OAAO,GACb,KAAK,OAAO,cACZ,KAAK,OAAO,QAAQ,MACpB,KAAK,QAAQ,QAAQ,OACrB,KAAK,QAAQ,QAAQ,OACrB,KAAK,OAAO,QAAQ;EACtB;AACF;ACnCA,SAAS,KAAK,MAAe;AAAA;AAE7B,SAAA,aAAA,WAAA;AAcO,MAAA,OAAS,aAAa,WAA+C,OAAA,IAAA,UAAA,sFAAA;AAC1E,QAAI,EAAA,UAAO,MAAc,UAAA,MAAA,UAAA,MAAA,UAAA,IAAA;AACvB,MAAA,iBAAU,IAAA,eAAA,MAAA,IAAA,OAAA,IAAA,YAAA;AAAA,WACR,KAAA,UAAA;AAAA,UAAA,QAAA,eAAA,SAAA,QAAA,iBAAA,EAAA,IAAA,UAAA,CAAA,UAAA,UAAA,IAAA,WAAA,GAAA,cAAA,GAAA,KAAA,EAAA;AAIE,eAAC,QAAU,SAAM,WAAgB,IAAA;AAEvC,qBAAI,YAEA,eACA;EAIJ;AAEE,WAAM,UAAQ,MAAA;AAMd,QAAA,SAAW,IAAA;AACT,oBAAc;AAGhB;IAEF;AAEA,QAAA,KAAS,WAAU,GAAc,GAAA;AAE/B,mBAAa,UAAI,KAAA,MAAA,KAAA,WAAA,IAAA,IAAA,IAAA,CAAA,CAAA;AACD;IACd;AACF,UAAA,sBAAA,KAAA,QAAA,GAAA;AAGI,QAAA,wBAAsB,IAAA;AACpB,YAAA,QACF,KAAA,MAAU,GAAK,mBAAW,GAAW,SAAQ,KAAM,sBAAA,CAAA,MAAA,MAAA,IAAA,GAAA,QAAA,KAAA,MAAA,sBAAA,MAAA;AAErD,mBAAA,OAAA,OAAA,IAAA;AACF;IAGM;AACN,iBAAI,MAAA,IAAA,IAAA;EAGF;AAQa,WAAA,aAAA,OAAO,OAAW,MAAA;AAC/B,YAAA,OAAA;MACF,KAAA;AAMa,oBAAA;AACf;MAES,KAAA;AAEP,eAAQ,GAAA,IAAO,GAAA,KAAA;;AAGC;MACZ,KAAA;AACF,aAAK,MAAA,SAAA,IAAA,IAAA,SAAA;AAGI;MAAe,KAAA;AACtB,gBAAA,KAAA,KAAA,IAAA,QAAA,SAAA,OAAA,EAAA,CAAA,IAAA,QAAA,IAAA,WAAA,6BAAA,KAAA,KAAA;UACF,MAAK;UAGH;UACA;QACF,CAAA,CAAA;AAIM;MAGF;AACwD,gBACpD,IAAM,WAAA,kBAAA,MAAA,SAAA,KAAA,GAAA,MAAA,MAAA,GAAA,EAAA,CAAA,WAAA,KAAA,KAAA;UAAA,MACN;UAAA;UACA;UACD;QAGL,CAAA,CAAA;AACF;IAEE;EAAA;AACM,WAAA,gBACF;AAAsE,SAAA,SACrE,KAAM,QAAA;MAAmC;MAC5C,OAAA,aAAA;;;MAIR,MAAA,KAAA,SAAA;CAEA,IAAA,KAAA,MAAS,GAAA,EAAA,IAAA;IACgB,CAAA,GAAA,KAAK,QAAS,OAEnC,IAAQ,YAAA;EAAA;AACN,WACA,MAAO,UAAa,CAAA,GAAA;AAAA,sBAAA,QAAA,WAAA,UAAA,cAAA,GAAA,eAAA,MAAA,KAAA,QAAA,OAAA,IAAA,YAAA,IAAA,iBAAA;EAAA;AAAA,SAGpB;IAAwB;IAAwB;EAQtD;AAES;AACH,SAAA,WAAA,OAAkB;AASxB,QAAA,QAAA,CAAA;AAEO,MAAA,iBAAO,IAAA,cAAA;AAChB,SAAA,cAAA,MAAA,UAAA;AASA,UAAS,UAAW,MAAA,QAA8D,MAAA,WAAA,GAAA,UAAA,MAAA,QAAA;GAOhF,WAAM;AACF,QAAA,UAAA;AAGG,QAAA,YAAA,MAAc,YAAM,KAAQ,UAAA,KAAA,IAAA,SAAA,OAAA,IAAA,YAAA,KAAA,UAAA,UAAA,YAAA,OAAA,UAAA,UAAA,YAAA,IAAA;AAE3B,uBAAU,MAAM,MAAQ,WAAM;AACA;IAGpC,OAAI;AAWJ,YAVI,OAAA,MAAY,MAAM,aAEpB,OAAA;AAUiB,YAAA,KAAA,IAAA,GAAA,cAAY,UAAW,GAAA,MAAA,cAAA,CAAA,MAAA,QAAA,MAAA,WAAA,MAAA;KACxC;IAAA;EAEA;AACA,SAAA;IAI8D;IAGhE;EACF;AAEO;;;ACxKF,IAAM,0BAAN,cAAsC,gBAA4C;EACvF,YAAY,EAAC,SAAS,SAAS,UAAS,IAAmB,CAAA,GAAA;AACrD,QAAA;AAEE,UAAA;MACJ,MAAM,YAAY;AAChB,iBAAS,aAAa;UACpB,SAAS,CAAC,UAAA;AACR,uBAAW,QAAQ,KAAK;UAC1B;UACA,QAAQ,OAAO;AACT,wBAAY,cACd,WAAW,MAAM,KAAK,IACb,OAAO,WAAY,cAC5B,QAAQ,KAAK;UAIjB;UACA;UACA;QAAA,CACD;MACH;MACA,UAAU,OAAO;AACf,eAAO,KAAK,KAAK;MACnB;IAAA,CACD;EACH;AACF;;;ACpFO,IAAM,0BAAN,cAAsC,MAAA;EAC3C,YAAY,SAAiB;AAC3B,UAAM,+BAA+B,OAAA,EAAS;EAChD;AACF;AAEO,IAAM,kCAAN,cAAiD,wBAAA;EAC/C;EACP,YAAY,SAAiB,UAAc;AACzC,UAAM,OAAA;AACN,SAAK,WAAW;EAClB;AACF;;;ACkCA,eAAsB,WAAW,UAAyB;AACxD,QAAM,MAAM,IAAI,IAAI,GAAG,QAAA,uBAA+B;AACtD,QAAM,OAAO,MAAM,YAAY,KAAK,QAAW,QAAW,OAAO;IAC/D,eAAe,UAAU,UAAU,UAAU,EAAA;EAC/C,CAAA;AACA,SAAQ,MAAM,KAAK,KAAI;AACzB;AACA,gBAAuB,sBACrB,UACA,OACA,QACA,gBAA+B;AAE/B,QAAM,WAAW,MAAM,YACrB,MAAM;IAAE;IAAO,MAAM;IAAyB,QAAQ;IAAM;EAAS,CAAA,GACrE,KAAK,UAAU,MAAA,GACf,cAAA;AAEF,QAAM,OAAO,SAAS;AACtB,MAAI,QAAQ,MAAM;AAChB;EACF;AAEA,mBAAiB,SAAS,KAAK,YAAY,IAAI,kBAAA,CAAA,EAAqB,YAAY,IAAI,wBAAA,CAAA,GAA4B;AAC9G,UAAM,eAAe,KAAK,MAAM,MAAM,IAAI;AAC1C,UAAM;EACR;AACF;AA0BA,eAAsB,mBACpB,UACA,OACA,QACA,gBAA+B;AAE/B,QAAM,WAAW,MAAM,YACrB,MAAM;IAAE;IAAO,MAAM;IAAsB,QAAQ;IAAO;EAAS,CAAA,GACnE,KAAK,UAAU,MAAA,GACf,cAAA;AAGF,QAAM,eAAgB,MAAM,SAAS,KAAI;AACzC,SAAO;AACT;AAEA,eAAe,YACb,KACA,MACA,gBACA,gBAAgB,QAChB,UAAkC,CAAC,GAAC;AAEpC,MAAI;AACJ,MAAI;AACF,eAAW,MAAM,MAAM,KAAK;MAC1B,GAAG,kBAAkB,cAAA;MACrB,QAAQ;MACR,SAAS;QACP,gBAAgB;QAChB,GAAG;MACL;MACA;IACF,CAAA;AACA,QAAI,CAAC,SAAS,IAAI;AAChB,UAAI,UAA8B;AAClC,UAAI;AACF,cAAM,UAAW,MAAM,SAAS,KAAI;AACpC,kBAAU,QAAQ,OAAO;AACzB,YAAI,SAAS,OAAO,SAAS;AAC3B,qBAAW,IAAI,KAAK,UAAU,QAAQ,MAAM,OAAO,CAAA;QACrD;MACF,SAAS,IAAI;MAEb;AACA,YAAM,IAAI,MAAM,IAAI,SAAS,MAAM,IAAI,SAAS,UAAU,KAAK,OAAA,EAAS;IAC1E;EACF,SAASE,IAAG;AACV,YAAQ,IAAIA,EAAA;AACZ,UAAM,MAAM,IAAI,wBAAwB,iCAAiCA,GAAE,OAAO,EAAE;AACpF,QAAI,QAAQA,GAAE;AACd,UAAM;EACR;AACA,SAAO;AACT;AAEA,IAAM,WAAW;AAEjB,SAAS,MAAM,EACb,OACA,MACA,QACA,SAAQ,GAMT;AACC,QAAM,cAAc,MAAM,WAAU;AACpC,QAAM,MAAM,IAAI,IAAI,GAAG,QAAA,IAAY,WAAA,WAAsB,KAAA,IAAS,IAAA,EAAM;AACxE,MAAI,aAAa,OAAO,OAAO,SAAS,MAAM;AAC9C,MAAI,QAAQ;AACV,QAAI,aAAa,OAAO,OAAO,KAAA;EACjC;AACA,SAAO;AACT;AAOA,SAAS,kBAAkB,gBAA+B;AACxD,QAAM,eAAe,CAAC;AACtB,MAAI,gBAAgB,SAAS;AAC3B,UAAM,kBAAkB,IAAI,gBAAA;AAC5B,UAAM,SAAS,gBAAgB;AAC/B,eAAW,MAAM,gBAAgB,MAAK,GAAI,eAAe,OAAO;AAChE,iBAAa,SAAS;EACxB;AACA,SAAO;AACT;;;ACxLO,SAAS,aAAa,UAAiC;AAC5D,MAAI,SAAS,cAAc,SAAS,WAAW,SAAS,GAAG;AACzD,QAAI,SAAS,WAAW,SAAS,GAAG;AAClC,cAAQ,KACN,qBAAqB,SAAS,WAAW,MAAM,6HAA6H;IAEhL;AACA,QAAI,mBAAmB,SAAS,WAAW,CAAA,CAAE,GAAG;AAC9C,YAAM,IAAI,gCACR,GAAG,wBAAwB,QAAA,CAAA,IAC3B,QAAA;IAEJ;AACA,WAAO,QAAQ,QAAA;EACjB;AACA,MAAI,SAAS,gBAAgB;AAC3B,UAAM,IAAI,gCACR,uBAAuB,wBAAwB,QAAA,CAAA,IAC/C,QAAA;EAEJ;AACA,SAAO;AACT;AAKO,SAAS,QAAQ,UAAiC;AACvD,MAAI,SAAS,aAAa,CAAA,EAAG,SAAS,QAAQ,CAAA,GAAI,MAAM;AACtD,WAAO,SAAS,WAAW,CAAA,EAAG,QAAQ,MAAM,CAAA,EAAG;EACjD;AACA,MAAI,SAAS,aAAa,CAAA,EAAG,SAAS,QAAQ,CAAA,GAAI,cAAc;AAC9D,WAAO,SAAS,WAAW,CAAA,EAAG,QAAQ,MAAM,CAAA,EAAG;EACjD;AACA,SAAO;AACT;AAEA,IAAM,mBAAmC;EAAC;EAAc;;AAExD,SAAS,mBAAmB,WAAoB;AAC9C,SAAO,CAAC,CAAC,UAAU,gBAAgB,iBAAiB,SAAS,UAAU,YAAY;AACrF;AAEA,SAAS,wBAAwB,UAAiC;AAChE,MAAI,UAAU;AACd,OAAK,CAAC,SAAS,cAAc,SAAS,WAAW,WAAW,MAAM,SAAS,gBAAgB;AACzF,eAAW;AACX,QAAI,SAAS,gBAAgB,aAAa;AACxC,iBAAW,WAAW,SAAS,eAAe,WAAW;IAC3D;AACA,QAAI,SAAS,gBAAgB,oBAAoB;AAC/C,iBAAW,KAAK,SAAS,eAAe,kBAAkB;IAC5D;EACF,WAAW,SAAS,aAAa,CAAA,GAAI;AACnC,UAAM,iBAAiB,SAAS,WAAW,CAAA;AAC3C,QAAI,mBAAmB,cAAA,GAAiB;AACtC,iBAAW,gCAAgC,eAAe,YAAY;AACtE,UAAI,eAAe,eAAe;AAChC,mBAAW,KAAK,eAAe,aAAa;MAC9C;IACF;EACF;AACA,SAAO;AACT;;;AC/DA,eAAsB,6BACpB,KACA,UACA,KAAY;AAEZ,QAAM,CAAC,OAAO,SAAA,IAAa,SAAS,GAAA;AACpC,MAAI,aAAoC;AAExC,MAAI;AACF,qBAAiB,MAAM,sBAAsB,UAAU,OAAO,SAAA,GAAY;AACxE,YAAM,OAAO,aAAa,EAAA;AAC1B,UAAI,OAAO,SAAS,UAAU;AAC5B,sBAAc;MAChB,OAAO;AACL,qBAAa;AACb;MACF;IACF;EACF,SAAS,KAAK;AAEZ,SAAK,MAAM,GAAA;AACX,SAAK,MAAM,KAAK,WAAW,IAAI,SAAQ,CAAA;AACvC,iBAAa,KAAK,WAAW,IAAI,SAAQ;EAC3C;AAEA,OAAK,MAAM,GAAA;AACX,OAAK,MAAM,UAAA;AAEX,WAAS,cAAc,SAA8B;AACnD,QAAI,OAAO,YAAY,UAAU;AAC/B,aAAO;QACL,IAAI;QACJ,QAAQ;QACR,SAAS,KAAK,MAAM,KAAK,IAAG,IAAK,GAAA;QACjC,OAAO,MAAM;QACb,SAAS;UACP;YACE,SAAS;cAAE,MAAM;cAAa;cAAkB,SAAS;YAAK;YAC9D,eAAe;YACf,OAAO;YACP,UAAU;UACZ;;MAEJ;IACF;AAEA,WAAO;MACL,IAAI;MACJ,QAAQ;MACR,SAAS,KAAK,MAAM,KAAK,IAAG,IAAK,GAAA;MACjC,OAAO,MAAM;MACb,SAAS;QACP;UACE,SAAS;YACP,MAAM;YACN,SAAS;YACT,SAAS;YACT,eAAe;cACb,MAAM,QAAQ,QAAQ;cACtB,WAAW,KAAK,UAAU,QAAQ,IAAI;YACxC;UACF;UACA,eAAe;UACf,OAAO;UACP,UAAU;QACZ;;IAEJ;EACF;AAEA,SAAO,SAAS,KAAK,cAAc,UAAA,CAAA;AACrC;;;ACvEO,SAAS,0BACd,KACA,UACA,KAAY;AAEZ,QAAM,CAAC,OAAO,SAAA,IAAa,SAAS,GAAA;AACpC,OAAK,MAAM,iCAAiC,GAAA;AAC5C,SAAO,YACJ,mBAAA;AACC,QAAI;AACF,uBAAiB,MAAM,sBAAsB,UAAU,OAAO,SAAA,GAAY;AACxE,aAAK,MAAM,8BAA8B,EAAA;AACzC,cAAM,OAAO,aAAa,EAAA;AAC1B,cAAM,cAAc;UAClB,OAAO,MAAM;UACb,SAAS;UACT,MAAM;QACR,CAAA;MACF;IACF,SAAS,OAAO;AACd,YAAM,cAAc;QAClB,OAAO,MAAM;QACb,SAAS,OAAO,WAAW,MAAM,SAAQ;QACzC,MAAM;MACR,CAAA;IACF;AACA,UAAM,cAAc;MAAE,OAAO,MAAM;MAAO,SAAS;MAAI,MAAM;IAAK,CAAA;AAClE,UAAM;AACN,WAAO;EACT,EAAA,CAAA;AAEJ;AAEA,SAAS,cAAc,EACrB,OACA,SACA,KAAI,GAKL;AACC,MAAI,OAAO,YAAY,UAAU;AAC/B,WAAO;MACL,IAAI;MACJ,QAAQ;MACR,SAAS,KAAK,MAAM,KAAK,IAAG,IAAK,GAAA;MACjC;MACA,SAAS;QACP;UACE,OAAO;YAAE,MAAM;YAAa;UAAQ;UACpC,eAAe,OAAO,SAAS;UAC/B,OAAO;QACT;;IAEJ;EACF;AAEA,SAAO;IACL,IAAI;IACJ,QAAQ;IACR,SAAS,KAAK,MAAM,KAAK,IAAG,IAAK,GAAA;IACjC;IACA,SAAS;MACP;QACE,OAAO;UAAE,MAAM;UAAa,eAAe;QAAQ;QACnD,eAAe,OAAO,kBAAkB;QACxC,OAAO;MACT;;EAEJ;AACF;AAEA,IAAM,UAAU,IAAI,YAAA;AAEpB,SAAS,YAAY,YAA+E;AAClG,QAAM,IAAI,IAAI,eAA2B;IACvC,MAAM,KAAK,YAAU;AACnB,YAAM,EAAE,OAAO,KAAI,IAAK,MAAM,WAAW,KAAI;AAC7C,UAAI,MAAM;AACR,mBAAW,MAAK;MAClB,OAAO;AACL,cAAM,OAAO,OAAO,UAAU,WAAW,QAAQ,KAAK,UAAU,KAAA;AAChE,mBAAW,QAAQ,QAAQ,OAAO,SAAS;UAAE;QAAK,CAAA,CAAA,CAAA;MACpD;IACF;EACF,CAAA;AAEA,QAAM,WAAW,IAAI,SAAS,GAAG;IAC/B,QAAQ;IACR,SAAS,IAAI,QAAQ;MACnB,gBAAgB;IAClB,CAAA;EACF,CAAA;AAEA,SAAO;AACT;AAEO,SAAS,SAAS,EAAE,OAAO,MAAM,GAAE,GAAY;AACpD,MAAI,SAAS,SAAS,IAAA;;AACtB,MAAI,OAAO;AACT,cAAU,UAAU,SAAS,EAAA;;EAC/B;AACA,MAAI,IAAI;AACN,cAAU,OAAO,MAAM,EAAA;;EACzB;AACA,SAAO,GAAG,MAAA;;AACZ;;;AC7GA,eAAsB,iBAAiB,QAAe;AACpD,QAAM,MAAO,MAAM,OAAO,KAAI;AAC9B,QAAM,UAAU,OAAO;AACvB,QAAM,WAAW,SAAS,OAAA;AAC1B,MAAI,YAAY,MAAM;AACpB,WAAO,IAAI,SAAS,gBAAgB;MAAE,QAAQ;IAAI,CAAA;EACpD;AAEA,MAAI,IAAI,WAAW,MAAM;AACvB,WAAO,MAAM,6BAA6B,KAAK,UAAU,OAAO,MAAM;EACxE;AACA,SAAO,0BAA0B,KAAK,UAAU,OAAO,MAAM;AAC/D;;;ACVA,IAAM,yBAAyB;AAK/B,IAAM,aAAa;AAInB,eAAsB,sBAAsB,QAAe;AACzD,QAAM,MAAO,MAAM,OAAO,KAAI;AAC9B,QAAM,MAAM,OAAO;AACnB,QAAM,UAAU,OAAO;AACvB,QAAM,WAAW,SAAS,OAAA;AAC1B,MAAI,YAAY,MAAM;AACpB,WAAO,IAAI,SAAS,gBAAgB;MAAE,QAAQ;IAAI,CAAA;EACpD;AAGA,QAAM,SAAS;IAAC,IAAI;IAAO,KAAI,EAAG,IAAI,CAAC,OAAO,GAAG,SAAQ,CAAA;AACzD,QAAM,gBAA4B,CAAA;AAElC,QAAM,kBAAkB,UAAU,sBAAA;AAClC,MAAI;AAEF,aAAS,IAAI,GAAG,IAAI,OAAO,QAAQ,KAAK,YAAY;AAClD,YAAM,cAAc,OAAO,MAAM,GAAG,IAAI,UAAA;AAExC,WAAK,KAAK,uBAAuB,YAAY,MAAM,iCAAiC,CAAA,GAAI;AAGxF,YAAM,eAA0C;;QAE9C,UAAU,YAAY,IAAI,CAAC,UAAU;UACnC,OAAO;UACP,SAAS;YACP,OAAO;cAAC;gBAAE;cAAK;;UACjB;QACF,EAAC;MACH;AAGA,YAAM,WAAW,MAAM,mBAAmB,UAAU,IAAI,YAAY,sBAAA,GAAyB,YAAA;AAG7F,UAAI,SAAS,YAAY;AACvB,cAAM,qBAAqB,SAAS,WAAW,IAAI,CAAC,QAAQ,IAAI,MAAM;AACtE,sBAAc,KAAI,GAAI,kBAAA;MACxB;IACF;AAGA,UAAM,eAAe,cAAc,IAAI,CAAC,WAAW,WAAW;MAC5D,QAAQ;MACR;MACA;IACF,EAAC;AAED,UAAM,gBAA2D;MAC/D,QAAQ;MACR,MAAM;MACN,OAAO,IAAI;MACX,OAAO;QACL,eAAe;QACf,cAAc;MAChB;IACF;AAEA,WAAO,SAAS,KAAK,aAAA;EACvB,SAAS,KAAK;AACZ,SAAK,MAAM,mBAAmB,GAAA;AAC9B,SAAK,MAAM,kBAAkB,KAAK,WAAW,IAAI,SAAQ,CAAA;AACzD,WAAO,IAAI,SAAS,KAAK,UAAU;MAAE,OAAO;QAAE,SAAS,IAAI;MAAQ;IAAE,CAAA,GAAI;MACvE,QAAQ;MACR,SAAS;QAAE,gBAAgB;MAAmB;IAChD,CAAA;EACF;AAkDF;;;AClIO,IAAM,YAAmC,OAAO,KAAK,YAAA,EAAc,IAAI,CAAC,WAAW;EACxF,SAAS;EACT,QAAQ;EACR,UAAU;EACV,IAAI;AACN,EAAC;AAEM,IAAM,SAAS,OAAO,QAAA;AAC3B,QAAM,WAAW,SAAS,IAAI,OAAO;AACrC,QAAM,gBAAgB,MAAM,WAAW,QAAA;AACvC,QAAM,aAAa,cAAc,MAAA;AACjC,QAAM,oBAAoB,WAAW,OAAO,SAAA;AAE5C,SAAO;IACL,QAAQ;IACR,MAAM,mBAAmB,mBAAmB,CAAC,SAAS,KAAK,EAAE;EAC/D;AACF;AAEA,SAAS,mBAAsB,KAAU,aAA6B;AACpE,QAAM,OAAO,oBAAI,IAAA;AACjB,SAAO,IAAI,OAAO,CAAC,SAAA;AACjB,UAAM,QAAQ,YAAY,IAAA;AAC1B,QAAI,KAAK,IAAI,KAAA,GAAQ;AACnB,aAAO;IACT;AACA,SAAK,IAAI,KAAA;AACT,WAAO;EACT,CAAA;AACF;AAEO,IAAM,cAAc,CAAC,UAAA;AAC1B,SAAO,UAAU,KAAK,CAAC,OAAO,GAAG,OAAO,KAAA;AAC1C;;;ACzBA,IAAM,EAAE,WAAW,QAAO,IAAK,EAAK;EAAE,cAAc;AAAI,CAAA;AAExD,IAAM,MAAM,EAAkC;EAC5C,QAAQ;IACN;IACA,CAAC,QAAA;AACC,UAAI,SAAS,IAAI,OAAO,OAAO,WAAU,EAAG,SAAQ,CAAA;AACpD,UAAI,OAAO,KAAK,OAAO,IAAI,MAAM,IAAI,IAAI,GAAG,EAAE;IAChD;;EAEF,SAAS;IACP;IACA,CAAC,GAAG,QAAA;AACF,UAAI,QAAQ,KAAK,OAAO,IAAI,MAAM,IAAI,IAAI,GAAG,EAAE;IAEjD;;AAEJ,CAAA;AAEA,IAAI,IAAI,KAAK,KAAA;AACb,IAAI,KAAK,wBAAwB,gBAAA;AACjC,IAAI,KAAK,oBAAoB,eAAA;AAC7B,IAAI,KAAK,kBAAkB,qBAAA;AAC3B,IAAI,IAAI,cAAc,OAAO,QAAQ,SAAS,KAAK,MAAM,OAAO,GAAA,CAAA,CAAA;AAChE,IAAI,IAAI,qBAAqB,CAAC,MAAM,SAAS,KAAK,YAAY,EAAE,OAAO,KAAK,CAAA,CAAA;AAC5E,IAAI,KAAK,4CAA4C,WAAA;AACrD,IAAI,IAAI,KAAK,MAAM,IAAI,SAAS,kBAAkB;EAAE,QAAQ;AAAI,CAAA,CAAA;;;ACnChE,QAAQ,IAAI,qCAAA;AAGZ,IAAI,MAAM;EACR,MAAM;EACN,OAAO,IAAI;AACb,CAAA;",
  "names": ["cors", "options", "e", "Router", "base", "r", "routes", "other", "__proto__", "DEFAULT_AUDIO_FORMAT", "VOICE_LIST", "e"]
}
